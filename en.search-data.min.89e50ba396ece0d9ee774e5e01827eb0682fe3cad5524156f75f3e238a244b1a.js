"use strict";(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/docs/example/",title:"Example Site",section:"Docs",content:`Introduction#Ferre hinnitibus erat accipitrem dixi Troiae tollens#Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.
Pedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret Est simul fameque tauri qua ad#Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.
Me sol#Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.
if (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue,viralItunesBalancing, bankruptcy_file_pptp)) {file += ip_cybercrime_suffix;}if (runtimeSmartRom == netMarketingWord) {virusBalancingWin *= scriptPromptBespoke + raster(post_drive,windowsSli);cd = address_hertz_trojan;soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui);} else {megabyte.api = modem_flowchart - web + syntaxHalftoneAddress;}if (3 \u0026lt; mebibyteNetworkAnimated) {pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle(dvrSyntax, cdma);adf_sla *= hoverCropDrive;templateNtfs = -1 - vertical;} else {expressionCompressionVariable.bootMulti = white_eup_javascript(table_suffix);guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1,managementRosetta(webcamActivex), 740874);}var virusTweetSsl = nullGigo;Trepident sitimque#Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.
Tamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.
`}),e.add({id:1,href:"/docs/example/table-of-contents/with-toc/",title:"With ToC",section:"Table of Contents",content:`Caput vino delphine in tamen vias#Cognita laeva illo fracta#Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.
Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit Natus quaerere#Pectora et sine mulcere, coniuge dum tincta incurvae. Quis iam; est dextra Peneosque, metuis a verba, primo. Illa sed colloque suis: magno: gramen, aera excutiunt concipit.
Phrygiae petendo suisque extimuit, super, pars quod audet! Turba negarem. Fuerat attonitus; et dextra retinet sidera ulnas undas instimulat vacuae generis? Agnus dabat et ignotis dextera, sic tibi pacis feriente at mora euhoeque comites hostem vestras Phineus. Vultuque sanguine dominoque metuit risi fama vergit summaque meus clarissimus artesque tinguebat successor nominis cervice caelicolae.
Limitibus misere sit#Aurea non fata repertis praerupit feruntur simul, meae hosti lentaque citius levibus, cum sede dixit, Phaethon texta. Albentibus summos multifidasque iungitur loquendi an pectore, mihi ursaque omnia adfata, aeno parvumque in animi perlucentes. Epytus agis ait vixque clamat ornum adversam spondet, quid sceptra ipsum est. Reseret nec; saeva suo passu debentia linguam terga et aures et cervix de ubera. Coercet gelidumque manus, doluit volvitur induta?
Enim sua#Iuvenilior filia inlustre templa quidem herbis permittat trahens huic. In cruribus proceres sole crescitque fata, quos quos; merui maris se non tamen in, mea.
Germana aves pignus tecta#Mortalia rudibusque caelum cognosceret tantum aquis redito felicior texit, nec, aris parvo acre. Me parum contulerant multi tenentem, gratissime suis; vultum tu occupat deficeret corpora, sonum. E Actaea inplevit Phinea concepit nomenque potest sanguine captam nulla et, in duxisses campis non; mercede. Dicere cur Leucothoen obitum?
Postibus mittam est nubibus principium pluma, exsecratur facta et. Iunge Mnemonidas pallamque pars; vere restitit alis flumina quae quoque, est ignara infestus Pyrrha. Di ducis terris maculatum At sede praemia manes nullaque!
`}),e.add({id:2,href:"/docs/example/table-of-contents/without-toc/",title:"Without ToC",section:"Table of Contents",content:`At me ipso nepotibus nunc celebratior genus#Tanto oblite#Lorem markdownum pectora novis patenti igne sua opus aurae feras materiaque illic demersit imago et aristas questaque posset. Vomit quoque suo inhaesuro clara. Esse cumque, per referri triste. Ut exponit solisque communis in tendens vincetis agisque iamque huic bene ante vetat omina Thebae rates. Aeacus servat admonitu concidit, ad resimas vultus et rugas vultu dignamque Siphnon.
Quam iugulum regia simulacra, plus meruit humo pecorumque haesit, ab discedunt dixit: ritu pharetramque. Exul Laurenti orantem modo, per densum missisque labor manibus non colla unum, obiectat. Tu pervia collo, fessus quae Cretenque Myconon crate! Tegumenque quae invisi sudore per vocari quaque plus ventis fluidos. Nodo perque, fugisse pectora sorores.
Summe promissa supple vadit lenius#Quibus largis latebris aethera versato est, ait sentiat faciemque. Aequata alis nec Caeneus exululat inclite corpus est, ire tibi ostendens et tibi. Rigent et vires dique possent lumina; eadem dixit poma funeribus paret et felix reddebant ventis utile lignum.
Remansit notam Stygia feroxque Et dabit materna Vipereas Phrygiaeque umbram sollicito cruore conlucere suus Quarum Elis corniger Nec ieiunia dixit Vertitur mos ortu ramosam contudit dumque; placabat ac lumen. Coniunx Amoris spatium poenamque cavernis Thebae Pleiadasque ponunt, rapiare cum quae parum nimium rima.
Quidem resupinus inducto solebat una facinus quae#Credulitas iniqua praepetibus paruit prospexit, voce poena, sub rupit sinuatur, quin suum ventorumque arcadiae priori. Soporiferam erat formamque, fecit, invergens, nymphae mutat fessas ait finge.
Baculum mandataque ne addere capiti violentior Altera duas quam hoc ille tenues inquit Sicula sidereus latrantis domoque ratae polluit comites Possit oro clausura namque se nunc iuvenisque Faciem posuit Quodque cum ponunt novercae nata vestrae aratra Ite extrema Phrygiis, patre dentibus, tonso perculit, enim blanda, manibus fide quos caput armis, posse! Nocendo fas Alcyonae lacertis structa ferarum manus fulmen dubius, saxa caelum effuge extremis fixum tumor adfecit bella, potentes? Dum nec insidiosa tempora tegit spirarunt. Per lupi pars foliis, porreximus humum negant sunt subposuere Sidone steterant auro. Memoraverit sine: ferrum idem Orion caelum heres gerebat fixis?
`}),e.add({id:3,href:"/docs/example/table-of-contents/",title:"Table of Contents",section:"Example Site",content:`Ubi loqui#Mentem genus facietque salire tempus bracchia#Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.
Rapit vocant si hunc siste adspice#Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!
Locus fuit caecis#Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.
iscsi_virus = pitch(json_in_on(eupViral),northbridge_services_troubleshooting, personal(firmware_rw.trash_rw_crm.device(interactive_gopher_personal,software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel,mips_whitelist_duplex, cpa)));if (5) {managementNetwork += dma - boolean;kilohertz_token = 2;honeypot_affiliate_ergonomics = fiber;}mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet(analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet),gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork,trim_duplex_file), cellTapeDirect, token_tooltip_mashup(ripcordingMashup)));module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) +coreLog.joystick(componentUdpLink), windows_expansion_touchscreen);bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling(ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);Placabilis coactis nega ingemuit ignoscat nimia non#Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.
if (ad_api) {zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive;gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop),panel_point_firmware);spyware_bash.statePopApplet = express_netbios_digital(insertion_troubleshooting.brouter(recordFolderUs), 65);}recursionCoreRay = -5;if (hub == non) {portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard),font_radcab, guidCmsScalable + reciprocalMatrixPim);left.bug = screenshot;} else {tooltipOpacity = raw_process_permalink(webcamFontUser, -1);executable_router += tape;}if (tft) {bandwidthWeb *= social_page;} else {regular += 611883;thumbnail /= system_lag_keyboard;}Caesorum illa tu sentit micat vestes papyriferi#Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.
Venasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.
`}),e.add({id:4,href:"/docs/example/collapsed/",title:"Collapsed",section:"Example Site",content:`Collapsed Level of Menu#Cognita laeva illo fracta#Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.
Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit `}),e.add({id:5,href:"/posts/les-fichiers-meta/",title:"Les fichiers .meta",section:"Articles",content:`Comme précisé dans l\u0026rsquo;article intitulé Utiliser Git avec Unity :
il est indispensable d\u0026rsquo;utiliser un Version Control System lorsque l\u0026rsquo;on travaille sur un projet en Unity.
On remarque cependant que certains fichiers générés doivent tout de même être versionnés, en particulier les fichier .meta1. Ceci est contre intuitif et semble aller à l\u0026rsquo;encontre de la philosophie de Git.
Les fichiers .meta servent en fait à stocker des metadonnées concernant les Assets importés dans les projets. En particulier, si on sélectionne un Asset dans la fenêtre Project, on obtient une fenêtre Inspector qui ressemble à ça : Tous ces paramètres permettent à Unity de savoir comment importer le fichier correspondant. Ces informations sont stockées dans le fichier .meta associé.
Les fichiers .meta stockent beaucoup d\u0026rsquo;autres informations. Pour le vérifier, il suffit d\u0026rsquo;en ouvrir un dans un éditeur de texte classique : Pour plus d\u0026rsquo;informations, la documentation de Unity1 détaille le fonctionnement interne de ces fichiers .meta, ainsi que certaines limitations.
Références#Asset Metadata, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:6,href:"/posts/commencer-avec-urp/",title:"Commencer avec URP",section:"Articles",content:`Le pipeline de rendu utilisé dans ce cours est le Universal Render Pipeline (ou URP)1. Il faut donc créer un projet Unity avec une configuration particulière pour pouvoir l\u0026rsquo;utiliser. Bien que la documentation Unity suggère d\u0026rsquo;utiliser le template fourni2 pour créer un projet basé sur URP, celui-ci contient beaucoup de choses dont nous n\u0026rsquo;aurons pas besoin. Ainsi, nous conseillons plutôt de commencer avec un projet vide (utilisant le Built-in Render Pipeline), puis de le configurer pour qu\u0026rsquo;il utilise URP3.
Transférer un projet existant vers URP
Il existe certaines limitations à utiliser URP. En particulier, ce pipeline de rendu utilise son propre système pour le post-processing. Dans ce cas, avant de transférer un projet vers URP, il faut dans un premier temps effacer le package de post-processing existant.Pour créer un projet utilisant URP, il faut donc suivre les étapes suivantes :
créer un projet vide en utilisant le template intitulé \u0026ldquo;3D\u0026rdquo; : ceci créera donc un projet basé sur le Built-in Render Pipeline; [optionnel] : rajouter un objet 3D dans la scène pour pouvoir rapidement vérifier que le changement de pipeline est effectif; installer le package \u0026ldquo;Universal RP\u0026rdquo; : créer un Asset de type Pipeline comme indiqué sur la figure suivante : configurer Unity pour qu\u0026rsquo;il utilise ce nouveau pipeline. Pour ce faire, il faut aller dans Edit/Project Settings\u0026hellip;/Graphics et dans le paramètre Scriptable Render Pipeline Settings, choisir le pipeline nouvellement créé. Unity devrait maintenant utiliser le nouveau pipeline de rendu. La conséquence directe est que les matériaux basés sur le Built-in Render Pipeline ne sont plus supportés. Ainsi, si vous avez des objets dans votre scène initiale, ils devraient avoir maintenant une couleur magenta. Si vous créez de nouveaux objets 3D, ils vont maintenant utiliser les nouveaux shaders compatibles avec URP. Ainsi, ils auront la couleur attendue comme visible sur la figure suivante (à gauche un cube créé avec l\u0026rsquo;ancien pipeline, à droite un cube créé avec URP) : Références#About the Universal Render Pipeline, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Using the Universal Render Pipeline in a new Project, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Installing the Universal Render Pipeline into an existing Project, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:7,href:"/posts/utiliser-git-avec_unity/",title:"Utiliser Git avec Unity",section:"Articles",content:`Il est indispensable d\u0026rsquo;utiliser un Version Control System lorsque l\u0026rsquo;on travaille sur un projet. En particulier, un VCS :
aide à travailler de manière collaborative; facilite la gestion des différentes versions du projet; permet de faire des tests en local, sans risquer de casser les versions existantes; autorise à revenir à une version précédente. Unity propose déjà son propre système appelé Unity Collaborate1. Il en existe beaucoup d\u0026rsquo;autres comme Perforce, Subversion, CVS, etc.
Nous allons nous concentrer sur le VCS le plus populaire à savoir Git2.
Lors de la création du projet Unity et du dépôt git associé, il faut mettre le fichier .gitignore à jour. Pour cela, on peut tout simplement utiliser le site gitignore.io 3: Il faut placer le fichier .gitignore à la racine du projet Unity : De plus, il faut configurer le projet Unity en allant dans le menu Edit / Project Settings / Version Control / Mode et sélectionner Visible Meta Files : Vous trouverez plus d\u0026rsquo;informations sur les fichiers .meta dans la documentation de Unity 4;
Enfin, il faut aller dans Edit / Project Settings / Editor, et pour le paramètre Asset Serialization, sélectionner Force Texte : Il est très compliqué de fusionner des fichiers de scènes et de Prefabs. Il est donc important d\u0026rsquo;éviter les merge conflicts pour ces fichiers. Pour ce faire, il est conseillé :
d\u0026rsquo;utiliser différentes scènes; d\u0026rsquo;utiliser les Prefabs autant que possible; d\u0026rsquo;utiliser les ScriptableObjects; et fréquemment faire des commits. Enfin, un projet Unity va régulièrement contenir de gros fichiers de données (.wav, .fbx, textures, etc.). Il est donc fortement recommandé d\u0026rsquo;utiliser git lfs 5.
À regarder (pour aller plus loin)#Références#Unity Collaborate, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Git\u0026#160;\u0026#x21a9;\u0026#xfe0e;
gitignore for Unity, gitignore.io\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Asset Metadata, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Git LFS, GitHub\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:8,href:"/posts/erreur-de-resolution_template_de_projet/",title:"Erreur de résolution de template de projet",section:"Articles",content:`Lors de la création d\u0026rsquo;un nouveau projet, Unity affiche parfois l\u0026rsquo;erreur \u0026ldquo;Failed to resolve project template\u0026rdquo; comme indiqué sur la capture suivante : Ceci est vraisemblablement causé par un bug 1 lorsque le path est trop long, ou contient des espaces. Il suffit donc de changer le nom du projet, du répertoire parent, ou de déplacer le projet.
Références#Failed to resolve project template: Failed to decompress, Forums Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:9,href:"/posts/nullreferenceexception_et_unassignedreferenceexception/",title:"Null Reference Exception et Unassigned Reference Exception",section:"Articles",content:`Dans Unity, quand vous accédez à une variable mal initialisée (ou pas initialisée du tout), vous récupérez une NullReferenceException1 ou une UnassignedReferenceException. Ces erreurs apparaissent dans la console de la façon suivante : Les erreurs les plus fréquentes sont :
utiliser une variable exposée dans l\u0026rsquo;Inspector mais non-assignée. Un simple drag and drop dans l\u0026rsquo;éditeur Unity suffit; utiliser une variable non-initialisée (contenant une référence nulle); mal épeler le nom d\u0026rsquo;un GameObject quand on utilise la fonction GameObject.Find()2; le Variable Shadowing3. Le code suivant montre certaines de ces erreurs : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 using System.Collections; using System.Collections.Generic; using UnityEngine; public class ExceptionsExample : MonoBehaviour { public GameObject unassignedRef1; [SerializeField] private GameObject unassignedRef2; private GameObject nullRef1; private GameObject nullRef2; private GameObject nullRef3; // Start is called before the first frame update private void Start() { // #1: not assigned in inspector Debug.Log(unassignedRef1.name); // #2: not assigned in inspector Debug.Log(unassignedRef2.name); // #3: not initialized Debug.Log(nullRef1.name); // #4: typo so Find() returns null var nullRef2 = GameObject.Find(\u0026#34;exceptionCubeWithTypo\u0026#34;); Debug.Log(nullRef2.name); // #5: shadowing, so ok here, but not ok in Shadowing() nullRef3 = gameObject; Debug.Log(nullRef3.name); Shadowing(); } private void Shadowing() { // #5: shadowing GameObject nullRef3 = gameObject; Debug.Log(nullRef3.name); } } À regarder (pour aller plus loin)#Références#Null Reference Exceptions, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
GameObject.Find, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Variable shadowing, Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:10,href:"/posts/les-events-dans-unity/",title:"Les Events dans Unity",section:"Articles",content:`Il existe plusieurs types d\u0026rsquo;Events utilisables dans Unity : les UnityEvents1 et les Events C# natifs2. Les Events offrent un moyen simple et efficace de construire un système de diffusion de messages. Ils suivent essentiellement le design pattern Observer3^.
Le seul avantage à utiliser les UnityEvents c\u0026rsquo;est qu\u0026rsquo;ils sont intégrés directement dans l\u0026rsquo;éditeur4. Cependant les UnityEvents sont beaucoup plus lents que les Events C# natifs5. En conclusion, utilisez les Events C# natifs de préférence.
À regarder (pour aller plus loin)#Références#UnityEvents, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Handling and raising events, Microsoft\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Observer, Game Programming Patterns\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Why choose UnityEvent over native C# events?, stackoverflow\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Event Performance: C# vs. UnityEvent, Jackson Dunstan\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:11,href:"/posts/les-pipelines-de-rendu-dans-unity/",title:"Les pipelines de rendu dans Unity",section:"Articles",content:` Le pipeline de rendu (graphics pipeline ou rendering pipeline en Anglais)1 représente la suite d\u0026rsquo;étapes à effectuer pour transformer une scène 3D en points 2D sur l\u0026rsquo;écran (pixels).
Les étapes principales du pipeline de rendu pour OpenGL sont : Unity propose 2 types de pipelines de rendu :
Built-in Render Pipeline2 : c\u0026rsquo;est le pipeline de rendu par défaut. Scriptable Render Pipeline (ou SRP)3 : c\u0026rsquo;est un pipeline de rendu configurable et pilotable grâce à des scripts C#. La vidéo suivante résume les principales différences des pipelines de rendu dans Unity : On peut en particulier noter les avantages suivants concernant SRP :
une plus grande flexibilité en ce qui concerne le rendu; un meilleur contrôle sur le rendu des objets et sur les performances en général. Mais SRP requiert beaucoup de développements (et donc de temps) avant d\u0026rsquo;avoir quelque chose de fonctionnel. De plus, chaque SRP est unique à chaque projet, et chaque plateforme.
Pour ces raisons, Unity offre 2 templates de SRP pré-construits et utilisables directement :
Universal Render Pipeline (ou URP, anciennement LWRP)4 : SRP pré-configuré pour fonctionner sur un maximum de plateformes (mobiles, consoles, etc.); High Definition Render Pipeline (ou HDRP)5 : SRP pré-configuré pour fournir les meilleurs résultats visuels sur les plateformes les plus performantes (PS 4/5, Xbox One/X/S, Computer with Metal or Vulkan support, etc.). Les sections suivantes résument les avantages et les désavantages des 3 pipelines de rendu fournis par Unity.
Built-in Render Pipeline#Ce pipeline de rendu devrait suffire pour la grande majorité des développeurs. Il existe beaucoup d\u0026rsquo;assets pré-existants (shaders, objets, scripts, etc.). Néanmoins, le Built-in Render Pipeline n\u0026rsquo;offre pas une grande liberté pour modifier le rendu. Enfin, Shader Graph6 et VFX Graph7 ne sont pas supportés.
URP : Universal Render Pipeline#URP offre de meilleures performances que le Built-in Render Pipeline. Il est compatible avec un grand nombre d\u0026rsquo;architectures. URP est recommandé pour les applications 2D, XR et les plateformes mobiles. De plus, URP remplacera à termes le Built-in Render Pipeline. Cependant, l\u0026rsquo;apprentissage est plus compliqué. Enfin, les outils pré-existants ne sont pas tous encore compatibles.
HDRP : Universal Render Pipeline#HDRP optimise les rendus pour les architectures haute performance comme les PCs et les consoles de jeux. Il offre plus de possibilités pour les shaders, l\u0026rsquo;illumination, le post-processing, etc. Enfin, HDRP supporte le \u0026ldquo;real-time Ray Tracing\u0026rdquo;8 et la technologie RTX. Cependant, tout comme URP, l\u0026rsquo;apprentissage est plus compliqué. De même, les outils pré-existants ne sont pas tous encore compatibles.
Références#Graphics Pipeline, Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Built-in Render Pipeline, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Scriptable Render Pipeline, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Universal Render Pipeline, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
High Definition Render Pipeline, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Shader Graph, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
VFX Graph, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Unity real-time Ray Tracing, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:12,href:"/docs/example/collapsed/3rd-level/4th-level/",title:"4th Level",section:"3rd Level",content:`4th Level of Menu#Caesorum illa tu sentit micat vestes papyriferi#Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.
Venasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.
`}),e.add({id:13,href:"/docs/example/collapsed/3rd-level/",title:"3rd Level",section:"Collapsed",content:`3rd Level of Menu#Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.
iscsi_virus = pitch(json_in_on(eupViral),northbridge_services_troubleshooting, personal(firmware_rw.trash_rw_crm.device(interactive_gopher_personal,software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel,mips_whitelist_duplex, cpa)));if (5) {managementNetwork += dma - boolean;kilohertz_token = 2;honeypot_affiliate_ergonomics = fiber;}mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet(analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet),gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork,trim_duplex_file), cellTapeDirect, token_tooltip_mashup(ripcordingMashup)));module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) +coreLog.joystick(componentUdpLink), windows_expansion_touchscreen);bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling(ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);`}),e.add({id:14,href:"/docs/example/hidden/",title:"Hidden",section:"Example Site",content:`This page is hidden in menu#Quondam non pater est dignior ille Eurotas#Latent te facies#Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.
Pater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona#O fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?
reality(burnDocking(apache_nanometer),pad.property_data_programming.sectorBrowserPpga(dataMask, 37,recycleRup));intellectualVaporwareUser += -5 * 4;traceroute_key_upnp /= lag_optical(android.smb(thyristorTftp));surge_host_golden = mca_compact_device(dual_dpi_opengl, 33,commerce_add_ppc);if (lun_ipv) {verticalExtranet(1, thumbnail_ttl, 3);bar_graphics_jpeg(chipset - sector_xmp_beta);}Fronde cetera dextrae sequens pennis voce muneris#Acta cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!
var runtimeDiskCompiler = home - array_ad_software;if (internic \u0026gt; disk) {emoticonLockCron += 37 + bps - 4;wan_ansi_honeypot.cardGigaflops = artificialStorageCgi;simplex -= downloadAccess;}var volumeHardeningAndroid = pixel + tftp + onProcessorUnmount;sector(memory(firewire + interlaced, wired)); `}),e.add({id:16,href:"/docs/shortcodes/buttons/",title:"Buttons",section:"Shortcodes",content:`Buttons#Buttons are styled links that can lead to local page or external link.
Example#{{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Get HomeContribute`}),e.add({id:17,href:"/docs/shortcodes/columns/",title:"Columns",section:"Shortcodes",content:`Columns#Columns help organize shorter pieces of content horizontally for readability.
{{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} Example#Left Content#Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.Mid Content#Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!Right Content#Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.`}),e.add({id:18,href:"/docs/shortcodes/details/",title:"Details",section:"Shortcodes",content:`Details#Details shortcode is a helper for details html5 element. It is going to replace expand shortcode.
Example#{{\u0026lt; details \u0026#34;Title\u0026#34; [open] \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}} {{\u0026lt; details title=\u0026#34;Title\u0026#34; open=true \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /details \u0026gt;}} TitleMarkdown content#Lorem markdownum insigne\u0026hellip;`}),e.add({id:19,href:"/docs/shortcodes/embeds/",title:"Embeds",section:"Shortcodes",content:`Embeds#Youtube#Vimeo#Code#199 200 201 202 203 204 205 206 207 208 209 210 211 // Corner case #4: sorting an array \u0026#34;reverse sorted\u0026#34; #if defined(USING_C_ARRAY) int arrayreversesorted[] = { 46, 45, 44, 43, 42 }; SELECTION_SORT_FUNCTION(arrayreversesorted, sizeof(arrayreversesorted) / sizeof(arrayreversesorted[0])); expectSorted(arrayreversesorted, sizeof(arrayreversesorted) / sizeof(arrayreversesorted[0])); printArray(arrayreversesorted, sizeof(arrayreversesorted) / sizeof(arrayreversesorted[0])); printf(\u0026#34;\\n\u0026#34;); // To get the same output as the C++ version #else auto arrayreversesorted = std::vector\u0026lt;int\u0026gt;{ 46, 45, 44, 43, 42 }; SELECTION_SORT_FUNCTION(arrayreversesorted.begin(), arrayreversesorted.end()); expectSorted(arrayreversesorted); std::cout \u0026lt;\u0026lt; arrayreversesorted \u0026lt;\u0026lt; std::endl; #endif `}),e.add({id:20,href:"/docs/shortcodes/expand/",title:"Expand",section:"Shortcodes",content:`Expand#Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.
Example#Default#{{\u0026lt; expand \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}} Expand↕Markdown content#Lorem markdownum insigne\u0026hellip;With Custom Label#{{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}} Custom Label...Markdown content#Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.`}),e.add({id:21,href:"/docs/shortcodes/hints/",title:"Hints",section:"Shortcodes",content:`Hints#Hint shortcode can be used as hint/alerts/notification block.
There are 3 colors to choose: info, warning and danger.
{{\u0026lt; hint [info|warning|danger] \u0026gt;}} **Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{\u0026lt; /hint \u0026gt;}} Example#Markdown content
Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus TelethusaMarkdown content
Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus TelethusaMarkdown content
Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa`}),e.add({id:24,href:"/docs/shortcodes/section/",title:"Section",section:"Shortcodes",content:`Section#Section renders pages in section as definition list, using title and description.
Example#{{\u0026lt; section \u0026gt;}} Page1Page 1#Page2Page 2#`}),e.add({id:25,href:"/docs/shortcodes/section/page1/",title:"Page1",section:"Section",content:`Page 1#`}),e.add({id:26,href:"/docs/shortcodes/section/page2/",title:"Page2",section:"Section",content:`Page 2#`}),e.add({id:27,href:"/docs/shortcodes/tabs/",title:"Tabs",section:"Shortcodes",content:`Tabs#Tabs let you organize content by context, for example installation instructions for each supported platform.
{{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}} # MacOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} Example#MacOSMacOS#This is tab MacOS content.
Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.
LinuxLinux#This is tab Linux content.
Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.
WindowsWindows#This is tab Windows content.
Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.
`}),e.add({id:29,href:"/docs/cours/00-introduction/",title:"Chapitre 0 : introduction",section:"Docs",content:`Chapitre 0 : introduction#Unity1 est une plateforme de développement 2D / 3D temps-réelle. Elle permet de créer de nombreuses applications finales, comme par exemple :
des cinématiques 3D temps-réelles; des jeux vidéo 2D; des jeux vidéo 3D; des applications de réalité virtuelle, de réalité augmentée et de réalité mixte; des visualisations 3D temps-réelles pour l\u0026rsquo;architecture et l\u0026rsquo;industrie. Des exemples d\u0026rsquo;applications développées avec Unity sont disponibles dans la playlist suivante :
Le potentiel de la visualisation 3D temps-réelle en général est très important. En particulier, une étude de Forrester (avril 2020) commandée par Unity2 montre que :
sur 348 leaders de l\u0026rsquo;industrie, 19% y font déjà appel et parmi ceux-ci, 94% pensent l\u0026rsquo;utiliser encore plus dans le futur; 55% de ces leaders pensent en avoir besoin dans les 2 années à venir; 97% de ceux qui ne l\u0026rsquo;utilisent pas encore, pensent que ça modifiera profondément leur façon de produire et de travailler. Getting Started#Le Unity Hub3 est le point d\u0026rsquo;entrée pour utiliser Unity. C\u0026rsquo;est une application standalone qui facilite en particulier :
la gestion de votre compte Unity et des licences associées; la création des projets en utilisant des templates; la gestion des différentes versions installées de Unity. Pour créer un nouveau projet, il suffit de choisir le template voulu dans Unity Hub : Une fois un premier projet créé, l\u0026rsquo;idéal est de configurer l\u0026rsquo;intégration avec Visual Studio. Pour ce faire, il faut aller dans Edit/Preferences/External Tools/Visual Studio et modifier les paramètres comme indiqué sur la figure suivante : Ce que nous verrons#Les chapitres de ce cours d\u0026rsquo;introduction à Unity couvrent les sujets suivants :
concepts fondamentaux caméras matériaux illumination scripting animation 3D illumination globale post-processing shaders graph Le but de chacun de ces chapitres est de présenter une introduction sur chaque sujet et de donner des pistes pour des recherches futures.
Ce que nous NE verrons PAS#Peut-être encore plus important que ce que nous allons voir, c\u0026rsquo;est ce que nous ne verrons pas directement. Certains aspects sont simples et ne posent aucune difficulté pour l\u0026rsquo;apprentissage autonome, d\u0026rsquo;autres sont trop avancées pour être abordés durant ce cours, certains enfin sont soit en phase d\u0026rsquo;abandon, soit toujours en développement.
Les principaux sujets que nous n\u0026rsquo;aborderons pas sont donc en particulier (liste non-exhaustive) :
2D multiplayer \u0026amp; réseau path finding \u0026amp; IA Unity services (Analytics, Collaborate, Cloud build, etc.) physique et bien plus encore\u0026hellip; IMPORTANT
À la fin de chaque chapitre, l\u0026rsquo;étudiant doit comprendre les concepts principaux, les termes spécifiques, et surtout savoir où chercher des informations complémentaires en ligne.Slides#Références#Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
How real-time 3D (RT3D) experiences are transforming global industries, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Unity Hub, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:30,href:"/docs/cours/01_01-prise-en-main/",title:"Chapitre 1.1 : prise en main",section:"Docs",content:`Chapitre 1.1 : prise en main#Dans ce chapitre, nous considérons qu\u0026rsquo;un projet utilisant le pipeline de rendu Universal Render Pipeline1 est déjà créé : Pour des raisons pratiques, nous utilisons directement le template fourni par Unity. Cependant, il est en général conseillé de commencer avec URP en utilisant un projet vide.
Interface Générale#Lorsque vous démarrez votre projet pour la première fois, vous avez une fenêtre qui ressemble à ce qui suit : Dans les sections qui suivent, nous passons en revue les principales fenêtres de Unity.
Project Window#La fenêtre Project2 ressemble à ceci : Elle rassemble tout le contenu à votre disposition pour ce projet : on parle d\u0026rsquo;Assets. Un Asset peut être toute donnée (en général un fichier) utilisable dans votre projet (images, modèles 3D, animations, etc.). Il existe déjà beaucoup d\u0026rsquo;Assets disponibles dans Unity ou sur l\u0026rsquo;Asset Store. Ces Assets sont organisés grâce à une hiérarchie de répertoires pour un accès facilité.
Scene View#La Scene View est une vue interactive 3 et ressemble à ceci : Les principales commandes pour la navigation sont :
alt + bouton gauche | milieu | droit de la souris ainsi que la roulette pour naviguer en 3D; F pour centrer la caméra sur l\u0026rsquo;objet sélectionné; Shift + F pour verrouiller la vue sur l\u0026rsquo;objet sélectionné. Il existe beaucoup d\u0026rsquo;autres options consultables directement dans la documentation 4.
Quand on parle de 3D, un gizmo permet de modifier l\u0026rsquo;orientation, la position et parfois les échelles de l\u0026rsquo;objet auquel il est attaché. En particulier, le scene gizmo permet de manipuler la caméra courante : En faisant un clic droit sur le scene gizmo, on peut afficher les vues prédéfinies disponibles pour la caméra courante : Il est enfin possible de bloquer l\u0026rsquo;orientation et la perspective de la caméra courante en cliquant sur le cadenas en haut à droite du scene gizmo.
Voici un exemple de navigation en 3D en utilisant les commandes précédemment présentées : Game View#La Game View est la vue finale 3D de votre application 5 : Il est possible de contrôler l\u0026rsquo;état du jeu (Play | Pause | Step) en utilisant les boutons suivants : ATTENTION
tout changement à la scène en mode Play est temporaire et sera annulé lorsque l\u0026rsquo;on quitte l\u0026rsquo;application.Hierarchy Window#La fenêtre Hierarchy contient tous les GameObjects contenus dans votre application 6 : Cette fenêtre organise les éléments sous forme de hiérarchie. Elle est très utile pour sélectionner des GameObjects, les (re)parenter et les cacher.
Inspector Window#La fenêtre Inspector affiche les informations détaillées du GameObject sélectionné 7 : Elle est très utile pour voir et modifier :
les paramètres de GameObjects et de Components; les paramètres des Assets; la façon dont les Assets sont importés dans Unity; les variables publiques (ou sérialisées) des scripts. Toolbar#La Toolbar rassemble plusieurs boutons pour les outils les plus utilisés 8 : Build \u0026amp; Run#Pour générer l\u0026rsquo;exécutable final de votre application 3D, il faut aller dans le menu File / Build Settings (aussi atteignable avec le raccourcis Ctrl + Shift + B) : On obtient une fenêtre qui ressemble à celle qui suit : À NOTER
Par défaut, le module permettant de générer des applications 3D en WebGL n\u0026rsquo;est pas installé. Il faut donc d\u0026rsquo;abord l\u0026rsquo;installer depuis le Unity Hub.Une fois WebGL sélectionné, il suffit de cliquer sur Switch Platform. Ensuite, il ne reste plus qu\u0026rsquo;à cliquer sur Build and Run. Une fois l\u0026rsquo;application générée, vous devriez obtenir quelque chose comme ce qui suit : Slides#Références#Les pipelines de rendu dans Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
The Project Window, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
The Scene view, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Scene view navigation, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
The Game view, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
The Hierarchy window, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
The Inspector window, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
The Toolbar, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:31,href:"/docs/cours/01_02-gameobjects-components-et-transforms/",title:"Chapitre 1.2 : GameObjects, Components et Transforms",section:"Docs",content:`Chapitre 1.2 : GameObjects, Components et Transforms#Dans ce chapitre, nous allons succinctement voir les éléments fondamentaux qui constituent une scène dans Unity, à savoir les GameObjects, les Components et les Transforms.
GameObjects#Tout objet dans une application Unity est un GameObject1, comme par exemple un Mesh, une Camera ou une Light. Cependant, ils n\u0026rsquo;offrent aucune fonctionnalité à proprement parler. En effet, les GameObjects ne sont en fait que des Containers pour des Components. Il est important de noter que tout GameObject doit pouvoir être positionné dans la scène. Ainsi, ils contiennent tous au moins un Component de type Transform. En particulier, un Empty, qui représente donc un GameObject vide, contient un Component de type Transform.
En allant dans le menu GameObject / Create Empty, le GameObject ainsi créé ressemble à ce qui suit dans l\u0026rsquo;Inspector : Un GameObject est donc formé de la façon suivante : la zone contenant les paramètres relatifs au GameObject en lui-même; le Transform attaché à tout GameObject; le ou les Components représentant les véritables fonctionnalités du GameObject; Enfin, durant le développement d\u0026rsquo;un projet, il est très souvent utile de pouvoir rapidement désactiver des Components ou même des GameObjects. Ceci peut se faire simplement depuis l\u0026rsquo;Inspector en les activant ou les désactivant. Components#Comme dit précédemment, les véritables fonctionnalités d\u0026rsquo;un GameObject sont données par les Components2 qui lui sont associés.
Pour ajouter un Component à un GameObject 3, il faut aller dans le menu Component et choisir le Component désiré. Par exemple, pour pouvoir contrôler un GameObject avec de la physique, il faut lui rajouter le Component Rigidbody dans le menu Components / Physics / Rigidbody : On peut aussi rajouter un Component à un GameObject directement depuis l\u0026rsquo;Inspector en cliquant sur le bouton Add Component : Il suffit alors de rechercher le nom du Component que l\u0026rsquo;on souhaite rajouter, et Unity nous affiche automatiquement ceux disponibles.
Il est intéressant de noter que si l\u0026rsquo;on crée un Empty, et qu\u0026rsquo;on lui rajoute un Component de type Light, alors Unity le considère comme une Light à part entière et affiche le gizmo associé : Transforms#Le Transform 4 est certainement le Component le plus important pour un GameObject. Il permet de définir tous les paramètres de transformation du GameObject comme sa position, son orientation et ses échelles. Ce Component est tellement important qu\u0026rsquo;il n\u0026rsquo;est pas possible de le supprimer d\u0026rsquo;un GameObject. Les gizmos permettent de modifier un Transform simplement et rapidement : Les Transforms peuvent être imbriqués les uns dans les autres : on parle de Parenting ou de relation Parent-Child. Toute transformation contenue par à un Transform est automatiquement appliquée à tous ces enfants (Children). La vidéo suivante montre comment modifier un Transform, comment les parenter, et comment les séparer : Slides#Références#GameObjects, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Introduction to components, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Using Components, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Transforms, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:32,href:"/docs/cours/01_03-prefabs-et-assets/",title:"Chapitre 1.3 : Prefabs et Assets",section:"Docs",content:`Chapitre 1.3 : Prefabs et Assets#Les Prefab et les Assets servent à inclure du contenu dans votre application 3D.
Prefabs#Un Prefab1 peut être vu comme un template de GameObject. Un Prefab est donc réutilisable et permet de facilement instancier de multiples copies du même GameObject dans une scène. Un des gros avantages des Prefabs est qu\u0026rsquo;ils gardent chaque instance synchronisée avec eux-mêmes : tous les changements effectués sur le Prefab sont automatiquement répercutés sur leurs instances dans la scène. Conceptuellement, tout ce passe comme ce qui est montré dans l\u0026rsquo;image suivante : Les Prefabs sont indépendants des scènes et peuvent donc être utilisés pour créer des instances dans plusieurs scènes de l\u0026rsquo;application. Pour créer un Prefab à partir d\u0026rsquo;un GameObject dans la scène, il suffit de glisser-déposer ce GameObject depuis la fenêtre Hierarchy vers la fenêtre Project.
Les Prefabs sont identifiables dans la fenêtre Hierarchy par une icône de cube bleu à gauche de leur nom : Pour instancier un GameObject à partir d\u0026rsquo;un Prefab, il existe plusieurs méthodes. On peut en particulier le glisser-déposer depuis la fenêtre Project directement dans la scène ou dans la fenêtre Hierarchy. Il est aussi possible d\u0026rsquo;instancier des Prefabs directement depuis un script.
Éditer un Prefab#Un Prefab est modifiable de plusieurs manières. Lorsqu\u0026rsquo;un Prefab est modifié, toutes ses instances sont automatiquement mises à jour. Il n\u0026rsquo;est pas possible de changer la structure d\u0026rsquo;un Prefab, mais on peut lui rajouter des GameObjects.
Depuis l’Inspector#Il est possible d\u0026rsquo;éditer un Prefab depuis la fenêtre Inspector directement.
En mode d\u0026rsquo;édition de Prefab isolé 2#Pour passer en mode d\u0026rsquo;édition de Prefab (i.e. Prefab Mode) sans la scène (i.e. isolé), il suffit de sélectionner le Prefab dans la fenêtre Project puis de cliquer sur le bouton Open Prefab dans la fenêtre Inspector. Il est aussi possible de double-cliquer sur le Prefab dans la fenêtre Project. Unity ouvre ensuite le Prefab seul : le fond de la scène change et devient bleu pour indiquer que nous sommes dans un mode spécial.
En mode d\u0026rsquo;édition de Prefab avec le contexte 2#On passe en mode d\u0026rsquo;édition de Prefab (i.e. Prefab Mode) avec la scène (i.e. le contexte), en cliquant sur la flèche à droite de leur nom dans la fenêtre Hierarchy.
La vidéo suivante résume les points mentionnés ci-dessus : Instance Overrides#Il est possible de rajouter des variations particulières aux instances d’un Prefab. On parle alors d\u0026rsquo;Instance overrides3. Les instances modifiées de cette manière restent toujours connectées à leur Prefab d\u0026rsquo;origine (i.e. les modifications du Prefab sont répercutées sur l\u0026rsquo;instance), à l\u0026rsquo;exception des paramètres modifiés de l’instance qui ne sont plus mis à jour quand le Prefab est édité. Ces overrides sont indiqués en gras dans l’Inspector.
La figure suivante résume succinctement le fonctionnement des overrides pour les Prefabs : Nested Prefabs et Prefabs Variants#Il est possible d\u0026rsquo;imbriquer des Prefabs dans des Prefabs. On appelle ceci des Nested Prefabs4.
Des overrides peuvent aussi être convertis en Prefab pour créer des Prefab Variants5.
En programmation orientée objet, on parlerait de composition pour les Nested Prefabs et d\u0026rsquo;héritage pour les Prefab Variants.
Comme nous pouvons le deviner, les Prefabs offrent une très grande flexibilité. Cependant, il faut respecter quelques règles pour éviter que les scènes ne deviennent inutilement complexes et finissent par devenir difficilement maintenables. Ces règles sont très similaires à celles concernant la programmation en générale, à savoir :
il faut autant que possible rester simple; Il est impératif de choisir des noms explicites et adéquats pour les GameObjects, les Components (ceux que nous créons sous forme de scripts) ainsi que les Prefabs; il est fortement recommandé d\u0026rsquo;éditer les Prefabs en utilisant le Prefab Mode. Assets#Comme rapidement expliqué précédemment, un Asset représente toute donnée qui est disponible pour le projet. Ces Assets peuvent être de différents types6 comme des textures, des objets 3D, des sons ou des musiques. Ils sont soit :
créés directement dans unity comme les Animation Controller, les Audio Mixers, etc.; créés dans des logiciels externes (comme Blender7 par exemple) et importés en tant que fichiers (.fbx, .wav, .wav, etc.); ou encore achetés sur l\u0026rsquo;Asset Store8 et importés dans le projet. Les Assets sont stockés dans le répertoire Assets de votre projet. Unity va ainsi surveiller ce répertoire en permanence et détecter lorsqu\u0026rsquo;un Assets est ajouté, enlevé ou modifié. Par exemple, lorsqu\u0026rsquo;un fichier contenant un objet 3D est modifié, Unity va automatiquement le réimporter et va mettre votre scène active à jour. Ceci permet de refléter immédiatement les changements apportés à l\u0026rsquo;objet 3D.
À chaque Assets, Unity assigne un identifiant unique et lui associe un fichier .meta avec le même nom, et dans le même répertoire. Il faut noter que ces fichier .meta sont très importants et ne doivent pas être manipulés. Enfin, il est possible de modifier la manière dont Unity va importer vos Assets dans la scène en utilisant les Import Settings associés.
Pour aller plus loin#Slides#Références#Prefabs, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Editing a Prefab in Prefab Mode\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Instance overrides, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Nested Prefabs, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Prefab Variants, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Common types of assets, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Blender\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Asset Store, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:33,href:"/docs/cours/02_01-cameras/",title:"Chapitre 2.1 : caméras",section:"Docs",content:`Chapitre 2.1 : caméras#Les caméras spécifient comment les objets 3D sont projetés (transformés) sur l\u0026rsquo;écran 2D. Elles définissent donc quels objets 3D seront visibles par l\u0026rsquo;utilisateur final.
Built-in, Universal and High Definition Render Pipelines
Dans Unity, les Components de type Camera sont différents en fonction du Render Pipeline utilisé. Dans ce cours, nous nous focalisons sur URP. Des liens sur la documentation relative aux Cameras pour le Built-in Render Pipeline et pour le High Definition Render Pipeline se trouvent dans les références.
Dans Unity, les Cameras1 sont composées et représentées de la manière suivante : On peut en particulier noter :
le Gizmo de la Camera et son icône associée dans la scène 3D; le frustum de la Camera; une fenêtre de preview visible dans la scène 3D lorsque qu\u0026rsquo;une Camera est sélectionnée; le Component Camera qui contient les vraies fonctionnalités. Il est important de rappeler qu\u0026rsquo;une Camera est un GameObject et contient donc au moins un Transform.
De manière simplifiée, on peut imaginer qu\u0026rsquo;une caméra prend des entrées, effectue des calculs, et stocke les résultats dans des sorties :
les entrées : la liste d’objets 3D à projeter ainsi que les paramètres de la Camera; les sorties : l\u0026rsquo;image finale (Color Buffer) et les informations de profondeur (Depth Buffer ou Z-Buffer). Les paramètres les plus utiles du Component Camera sont :
Environment / Background Type : spécifie la couleur ou l\u0026rsquo;image du fond. C\u0026rsquo;est ce qui doit être affiché dans les espaces vides, c\u0026rsquo;est-à-dire les espaces de l\u0026rsquo;image finale dans lesquels aucun objet 3D n\u0026rsquo;est présent. Projection / Projection : définit le type de projection à effectuer. Elle peut-être perspective ou orthogonale. Projection / Field of View : correspond à l\u0026rsquo;angle de vue (horizontal ou vertical) de la Camera. Projection / Clipping Planes : ce paramètre permet d\u0026rsquo;ignorer les objets qui sont trop près ou au contraire trop éloignés. Output / Viewport Rect : permet d\u0026rsquo;afficher l\u0026rsquo;image finale sur une sous-partie de l\u0026rsquo;écran seulement. Certains paramètres offrent des fonctionnalités très intéressantes et méritent d\u0026rsquo;être un peu plus détaillés.
Culling Mask#Le paramètre Rendering / Culling Mask permet de spécifier quels groupes d\u0026rsquo;objets 3D doivent être considérés (et donc rendus) par la Camera. Un Culling Mask permet par exemple de créer des mini-maps avec une Camera qui fait le rendu de la scène 3D en vue du dessus, en ne considérant que certains objets particuliers. Par exemple, on peut souhaiter ignorer le personnage principal, mais uniquement utiliser une représentation simplifiée sur la mini-map.
Output Texture#Le paramètre Output / Output Texture permet de sauvegarder le rendu final dans une texture de rendu (Render Texture), plutôt que de l\u0026rsquo;afficher sur l\u0026rsquo;écran. Cette Render Texture peut ensuite être réutilisée en temps-réel dans la scène 3D.
Pour attacher une Render Texture à une caméra par l\u0026rsquo;intermédiaire du paramètre Output Texture, il faut d\u0026rsquo;abord la créer sous forme d\u0026rsquo;Asset. Pour ce faire, il faut aller dans le menu Assets/Create/Render Texture, comme indiqué sur l\u0026rsquo;image suivante : Les Render Textures sont utiles par exemple pour :
avoir plusieurs rendus de Camera dans la même vue (vue top, left, right de la même scène); simuler des caméras de surveillance. On positionne des Cameras là où il y a des caméras de surveillance dans la scène 3D. Ensuite, les images calculées par les {\u0026lt;unity_keyword Cameras\u0026gt;}} sont stockées en temps-réel dans des Render Texture. Ces textures sont finalement plaquées sur des objets 3D représentant des écrans dans la scène 3D. Slides#Références#Cameras, URP, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:34,href:"/docs/cours/02_02-materiaux/",title:"Chapitre 2.2 : matériaux",section:"Docs",content:`Chapitre 2.2 : matériaux#Les Materials1 définissent l\u0026rsquo;aspect visuel des GameObjects dans Unity. Ils contiennent donc les informations nécessaires pour calculer les rendus.
Un Material contient les éléments suivants : un Shader qui est chargé de faire les calculs pour le rendu à proprement parler. Nous verrons les Shaders dans un chapitre dédié au Shader Graph; des Textures2 qui représentent les données dont le Shader a besoin pour le rendu; des paramètres qui sont injectés au Shader pour contrôler le rendu. Il existe déjà beaucoup de Shaders et de Materials livrés avec Unity ou disponible directement sur l\u0026rsquo;Asset3 de Unity.
In-Editor Asset Store Unity 2020.1
Contrairement à ce qui est indiqué dans la documentation, l\u0026rsquo;Asset Store n\u0026rsquo;est plus disponible dans l\u0026rsquo;éditeur Unity depuis la version 2020.1. Il faut passer par le site web directement.Dans Unity, il existe plusieurs Render Pipelines. Dans le cadre de ce cours, nous utiliserons le Universal Render Pipeline (URP). Il faut donc faire attention de créer et d\u0026rsquo;utiliser des Shaders et des Materials compatibles.
Il existe plusieurs Shaders disponibles avec URP4. En particulier, le Lit Shader5 offre beaucoup de possibilités et permet très souvent d\u0026rsquo;obtenir les résultats souhaités. Dans les cas où le rendu final n\u0026rsquo;a pas besoin d\u0026rsquo;être proche de la réalité, ou sur des architectures limitées en puissance, alors le Simple Lit Shader6 est approprié.
Les figures suivantes montrent un exemple de Material utilisant le Lit Shader (à gauche), et le Simple Lit Shader (à droite ) :
Les Materials sont des Assets qui pourront être appliqués à plusieurs GameObjects différents à la fois. Pour les créer, il faut donc aller dans le menu Assets/Create/Material et choisir le Shader désiré. Il ne reste plus qu\u0026rsquo;à l\u0026rsquo;appliquer aux {\u0026lt;unity_keyword GameObjects \u0026gt;}} désirés. Pour ce faire, on peut :
éditer le Mesh Renderer Component; tout simplement faire un drag-and-drop sur l\u0026rsquo;objet 3D dans la scène. Slides#Références#Creating and Using Materials, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Textures, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Using the Asset Store, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Shaders and Materials, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Lit Shader, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Simple Lit Shader, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:35,href:"/docs/cours/02_03-illumination/",title:"Chapitre 2.3 : illumination",section:"Docs",content:`Chapitre 2.3 : illumination#Afin de voir une scène 3D et les objets qui la composent, il faut rajouter des lumières et les effets associés (ombres, brouillard, réflexions, etc).
Il existe 3 classes de lumières :
temps-réelles : elles sont calculées à l\u0026rsquo;exécution par Unity. Par conséquent, elles utilisent des techniques peu gourmandes en calculs. Ces lumières sont appropriées lorsque les objets de la scène sont dynamiques (changent à l\u0026rsquo;exécution); pré-calculées : on parle de Baked Lights. Ici, l\u0026rsquo;illumination est calculée avant l\u0026rsquo;exécution. Les résultats sont dans un premier stockés dans des Lightmaps, puis réutilisés en temps-réel. Les techniques utilisées sont plus complexes mais donnent des résultats plus réalistes. Ces lumières sont appropriées pour des scènes statiques c\u0026rsquo;est-à-dire qui ne changent pas à l\u0026rsquo;exécution; hybrides : enfin, il est possible de mélanger les 2 types de lumières selon les objets 3D considérés. Ce chapitre se focalise uniquement sur les lumières temps-réelles. Les techniques plus avancées permettant de faire des rendus photoréalistes seront abordées dans le chapitre dédié à l\u0026rsquo;illumination globale.
Light Component#Dans Unity, les Lights1 sont composées et représentées de la manière suivante : En particulier, on remarque :
le Gizmo de la Light et son icône associée dans la scène 3D; le Light Component avec les paramètres des lumières. Il faut noter qu\u0026rsquo;une Light est un GameObject, et donc contient aussi un Transform.
Pour créer une Light, il faut aller dans le menu GameObject \u0026gt; Light et choisir le type de lumière désiré, comme indiqué sur la figure suivante : Les Lights peuvent être de 4 types :
Directional : ici, seule l\u0026rsquo;orientation est importante; Point : pour ce type de lumières, seule la position a une incidence sur l\u0026rsquo;illumination; Spot : dans ce cas, modifier la position ou l\u0026rsquo;orientation de la lumière modifie aussi l\u0026rsquo;illumination; Area : ce type de lumières est particulier car c\u0026rsquo;est toute la surface qui a son importance. Ces lumières sont plus complexes à calculer et sont forcément Baked et non Realtime. Les principaux paramètres des Lights sont modifiables facilement grâce au Light Explorer atteignable par le menu Window \u0026gt; Rendering \u0026gt; Light Explorer. La figure suivante en donne un aperçu : Culling Mask#De manière similaire aux Cameras, le paramètre Culling Mask permet de spécifier quels groupes d\u0026rsquo;objets 3D doivent être considérés (rendus) par la Light. Les figures suivantes montrent la différence entre une lumière qui considère tous les objets 3D de la scène, et une lumière que ne considèrent qu\u0026rsquo;un sous-ensemble de ces objets. Tous les objets 3D
Seulement les objets 3D sélectionnés
Lumière environnementale#En plus des Light Components, l\u0026rsquo;environnement participe aussi à l\u0026rsquo;illumination et a un impact sur l\u0026rsquo;aspect final de la scène 3D. La fenêtre de configuration pour les différents paramètres de lumière environnementale est appelée la Lighting window2 et est accessible dans le menu Window \u0026gt; Rendering \u0026gt; Lighting comme indiqué sur la figure suivante : On obtient alors une fenêtre contenant les onglets Scene, Environment et Baked Lightmaps. En sélectionnant l\u0026rsquo;onglet Environment, la fenêtre affiche quelque chose de similaire à ce qui suit : Il y a en particulier plusieurs sections que nous détaillons dans la suite :
les Skybox; La lumière ambiante; les réflexions associées à l\u0026rsquo;environnement; et le brouillard dans la scène 3D. Les sous-sections suivantes introduisent ces notions à l\u0026rsquo;exception des réflexions qui sont, quant à elles, abordées dans un chapitre dédié.
Skybox#Une Skybox représente ce qui est visible à l\u0026rsquo;arrière-plan, quand il n\u0026rsquo;y a aucun objet 3D pour obstruer la vue.
URP vs HDRP
Le système de Skybox est différent selon qu\u0026rsquo;on utilise URP ou HDRP. Dans ce cours, nous nous focalisons sur URP.Dans Unity, une Skybox est en fait un Material avec un Shader de type Skybox. Une fois créée, cette Skybox peut être soit globale (donc partagée par tous les objets), ou locale pour chaque caméra. Si elle est globale, alors on peut rajouter le Material dans le paramètre Skybox Material de la fenêtre Lighting window. Si elle est locale à une caméra, alors il faut dans un premier temps rajouter un Component de type Skybox à la caméra, puis y associer la Skybox.
Lumière ambiante#La lumière ambiante (Ambient Light) est une lumière diffuse, qui vient de toutes les directions à la fois. Elle est très utile car Les techniques de rendus actuelles \u0026ldquo;perdent\u0026rdquo; en général un peu de lumière par rapport à la réalité. La lumière ambiante est donc une technique simple pour \u0026ldquo;rehausser\u0026rdquo; artificiellement le niveau de lumière dans la scène. Cette lumière permet aussi de rajouter des ambiances particulières en fonction de la teinte utilisée.
Brouillard#Le brouillard (Fog) dans une scène 3D offre plusieurs avantages :
il permet de cacher les éléments éloignés de la caméra, et donc d\u0026rsquo;éviter les effets de clipping (lorsqu\u0026rsquo;un objet dépasse soudainement le far clipping plane); il permet de mettre en évidence les objets qui sont aux premier-plan, par rapport aux éléments plus éloignés de la caméra. Ombres#Les ombres sont un élément essentiel de toute scène 3D. Ce sont elles qui donnent l\u0026rsquo;information de profondeur. En particulier, sans les ombres nous avons souvent l\u0026rsquo;impression que les objets flottent au lieu d\u0026rsquo;être posés sur une surface. La technique principale pour calculer les ombres est appelée Shadow Mapping Elle consiste à faire un pseudo-rendu de la scène en positionnant la caméra sur chacune des lumières. Ainsi, on peut calculer toutes les surfaces vues (et donc éclairées) pour chaque lumière. Le résultat de ce calcul correspond en fait à la Depth Map calculée durant le rendu. Dans le cas du calcul d\u0026rsquo;ombres, on parle plutôt de Shadow Map. Dans un deuxième temps, grâce aux différentes Shadow Maps calculées, on peut rapidement déduire les parties ombragées, et ceci pour chaque lumière.
Les ombres sont configurables de 3 manières différentes :
les paramètres de qualité des ombres. Ils sont globaux et appliqués à toute la scène; les paramètres liés à chaque objet; et enfin les paramètre liés à chaque lumière. Les paramètres de qualité des ombres sont stockés dans des Assets. En allant dans le répertoire Assets, on peut trouver plusieurs Assets contenant les paramètres de qualité pour le rendu. En particulier, en sélectionnant Universal Render Pipeline (le nom peut varier), la section concernant les ombres ressemble à la figure qui suit : En sélectionnant le GameObject qui nous intéresse, alors on peut consulter le Mesh renderer associé. En particulier, il contient les paramètres pour définir les ombres spécifiquement à ce GameObject comme indiqué sur la figure qui suit : Enfin, on peut configurer les ombres projetées pour chaque lumière. Pour ce faire, il faut aller dans le Light Component et modifier les paramètres présents : Slides#Références#Lights, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
The Lighting window, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:36,href:"/docs/cours/03-scripting/",title:"Chapitre 3 : scripting",section:"Docs",content:`Chapitre 3 : scripting#Dans Unity, l\u0026rsquo;intelligence de l\u0026rsquo;application 3D se programme grâce à des scripts C#. Lorsque ces scripts sont attachés à un GameObject, alors Unity les exécute en fonction des événements qui interviennent.
Hello World#Pour attacher un script à un objet dans la scène 3D, il faut sélectionner le GameObject puis cliquer sur le bouton Add Component. Ensuite, on renseigne le nom du script que l\u0026rsquo;on souhaite attacher. La figure suivante résume ce que vous devriez obtenir : S\u0026rsquo;il n\u0026rsquo;est pas présent, alors il est automatiquement créé en cliquant sur New Script puis sur Create and Add. Le script peut ensuite être ouvert dans votre IDE en double-cliquant dessus. Il ressemble à ce qui suit :
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 using System.Collections; using System.Collections.Generic; using UnityEngine; public class HelloWorld : MonoBehaviour { // Start is called before the first frame update void Start() { } // Update is called once per frame void Update() { } } Le script contient déjà 2 fonctions importantes : void Start() et void Update(). Comme indiqué dans les commentaires :
void Start() est appelée 1 seule fois, au début de l\u0026rsquo;exécution; void Update() est appelée à chaque frame. Il est très facile d\u0026rsquo;afficher un message à chaque appel à void Start() ainsi qu\u0026rsquo;à void Update(). Pour cela, il suffit de modifier le script pour rajouter un appel à la fonction Log() de la classe Debug1 comme indiqué dans le code qui suit :
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 using System.Collections; using System.Collections.Generic; using UnityEngine; public class HelloWorld : MonoBehaviour { // Start is called before the first frame update void Start() { Debug.Log(\u0026#34;Hello Start()\u0026#34;); } // Update is called once per frame void Update() { Debug.Log(\u0026#34;Hello Update()\u0026#34;); } } Lorsque l\u0026rsquo;on exécute l\u0026rsquo;application, on peut regarder ce qui est affiché dans la fenêtre Console : En particulier, on remarque bien que le message Hello Start() est affiché une seul fois au début, et que le message Hello Update() est affiché en continu, comme attendu.
Comme vous l\u0026rsquo;avez sans doute déjà remarqué, un Script est en fait un Component. Il doit donc être attaché à un GameObject pour être exécuté. De plus, il faut noter que les membres publics d\u0026rsquo;un Script sont automatiquement accessibles depuis l\u0026rsquo;éditeur de Unity. C\u0026rsquo;est très pratique lorsque l\u0026rsquo;on souhaite pouvoir modifier et tester rapidement des paramètres du script à l\u0026rsquo;exécution. Enfin, il faut remarquer que le Script hérite de la classe MonoBehavior. C\u0026rsquo;est la classe MonoBehavior qui gère en particulier le comportement général du Script (quand il est instancié, quelles fonctions sont appelées, quand elles sont appelées, etc.).
Constructor et Destructor
Unity gère lui-même la création et la destruction des Scripts. Il ne faut donc surtout pas implémenter les constructeurs au risque de perturber le fonctionnement complet de votre application.Event Functions#Les Scripts ont des points d\u0026rsquo;entrées (des fonctions) prédéfinis qui vont être automatiquement appelés par Unity en réponse à certains événements : ce sont les Event Functions2.
Remarque
les fonctions void Start() et void Update() sont des Event Functions.
L\u0026rsquo;ordre d\u0026rsquo;exécution des Event Functions est important. La figure suivante résume les principales Event Functions et surtout leur ordre d\u0026rsquo;exécution : Elles sont documentées dans la classe MonoBehavior3. Il faut noter qu\u0026rsquo;elles sont très nombreuses et que seul un sous-ensemble d\u0026rsquo;entre elles est en général utilisé. La vidéo suivante donne un aperçu de la séquence d\u0026rsquo;appel de l\u0026rsquo;ensemble des Event Functions4 : Time.DeltaTime#Il est important de garder à l\u0026rsquo;esprit que la fonction void Update() est appelée à chaque frame, c\u0026rsquo;est-à-dire aussi souvent que possible. Par conséquent, elle ne tient pas compte du temps écoulé entre 2 frames rendues. Ainsi, le résultat de la fonction void Update() peut dépendre du frame rate. Par exemple, lorsqu\u0026rsquo;elle chargée de déplacer un GameObject d\u0026rsquo;une certaine distance à chaque frame, alors :
si le frame rate est élevé (par exemple sur une machine puissante), elle sera appelée très souvent. Le GameObject se déplacera donc très vite. Au contraire, si le frame rate est faible, elle sera appelée moins souvent. Le GameObject se déplacera dans ce cas plus lentement. En général, avoir une application 3D dont le comportement dépend de la machine sur laquelle elle est exécutée n\u0026rsquo;est pas souhaitable. Pour corriger ce problème, il faut utiliser Time.DeltaTime qui nous renseigne sur le temps écoulé depuis le dernier appel à void Update(). Ainsi, il est facile de rendre l\u0026rsquo;exécution de cette fonction indépendante du frame rate : 1 2 3 4 5 6 public float speed = 5; void Update() { float distance = speed * Time.deltaTime; transform.Translate(Vector3.up * distance); } Modifier des GameObjects à l\u0026rsquo;exécution#Un script peut modifier les Components :
appartenant au même GameObject (celui auquel il est attaché); appartenant à d\u0026rsquo;autres GameObjects qu\u0026rsquo;il connaît au préalable; appartenant à d\u0026rsquo;autres GameObjects qui sont instanciés (et détruits) dynamiquement. Modifier les Components du même GameObject#Un Script a accès aux Components attachés au même GameObject en utilisant leur type : public Component GetComponent\u0026lt;Type\u0026gt;(); Ou plus rarement : public Component[] GetComponents\u0026lt;Type\u0026gt;(); Par exemple, pour récupérer le RigidBody attaché au même GameObject que le Script, on peut faire : 1 2 3 var rigidBody = GetComponent\u0026lt;Rigidbody\u0026gt;(); rigidBody.AddForce(new Vector3(0, 500, 0)); rigidBody.AddTorque(new Vector3(30, 30, 30)); Modifier les Components d\u0026rsquo;autres GameObjects connus#Il est possible pour un Script de modifier des Components appartenant à d\u0026rsquo;autres GameObjects. En particulier, si le GameObject est connu au préalable, et si le lien entre le Script et le GameObject (ou le Component) à interroger ou modifier est permanent, alors on peut tout simplement rajouter un membre public dans le Script du type du Component que l\u0026rsquo;on attend : 1 2 3 4 5 6 7 public class CheckProximityObserver : MonoBehaviour { public GameObject go1; public GameObject go2; ... } Comme les membres sont publics, alors ils sont directement visibles dans l\u0026rsquo;éditeur. Il suffit donc de faire un drag-and-drop des GameObjects que l\u0026rsquo;on souhaite utiliser : Ensuite, dans le Script, on peut tout simplement utiliser les GameObjects (ou les Components) liés de la manière suivante : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 void Update() { var pos1 = go1.transform.position; var pos2 = go2.transform.position; var distance = pos2 - pos1; if (distance.magnitude \u0026lt; maxDistance) { var m1 = go1.GetComponent\u0026lt;Renderer\u0026gt;().material; m1.color = new Color(1, 0, 0); var m2 = go2.GetComponent\u0026lt;Renderer\u0026gt;().material; m2.color = new Color(1, 0, 0); } else { var m1 = go1.GetComponent\u0026lt;Renderer\u0026gt;().material; m1.color = new Color(0, 0, 1); var m2 = go2.GetComponent\u0026lt;Renderer\u0026gt;().material; m2.color = new Color(0, 0, 1); } } Modifier les Components d\u0026rsquo;autres GameObjects créés dynamiquement#Si les GameObjects sont créés et détruits dynamiquement, alors il est préférable de tous les grouper en les parentant au même GameObject. Ce dernier va donc servir de conteneur, que l\u0026rsquo;on passe au script en utilisant un membre public. Il ne reste plus qu\u0026rsquo;à itérer sur tous les Components du conteneur ayant le type désiré : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public class CheckProximityObserver2 : MonoBehaviour { public Transform chairs; public float maxDistance; void Update() { // Iterate over all Transforms (children) contained in chairs group foreach (Transform t1 in chairs) { string name = transform.name; // First reset its color to ok var mat = t1.GetComponent\u0026lt;Renderer\u0026gt;().material; mat.color = new Color(0, 1, 0); foreach (Transform t2 in chairs) { if (t1 == t2) { continue; } var currentDistance = (t1.position - t2.position).magnitude; if (currentDistance \u0026lt; maxDistance) { // Then set color to error mat.color = new Color(1, 0, 0); break; } } } } } Recherche de GameObjects à l\u0026rsquo;exécution#Il est possible de rechercher des GameObjects à l\u0026rsquo;exécution en utilisant leur nom ou leur tag grâce au fonctions suivantes : public static GameObject Find(string name); public static GameObject FindWithTag(string tag); public static GameObject[] FindGameObjectsWithTag(string tag); Par exemple, il est possible de récupérer tous les GameObjects qui ont comme Tag \u0026ldquo;Cubes\u0026rdquo; : 1 2 3 4 5 6 7 8 9 var GOs = GameObject.Find(\u0026#34;Cube\u0026#34;); var firstCube = GameObject.FindWithTag(\u0026#34;Cubes\u0026#34;); var allCubes = GameObject.FindGameObjectsWithTag(\u0026#34;Cubes\u0026#34;); foreach(GameObject go in allCubes) { // Do great things here } Instancier et détruire des GameObjects à l\u0026rsquo;exécution#Il est possible d\u0026rsquo;instancier des GameObjects à l\u0026rsquo;exécution en utilisant la méthode Instantiate5 : public static Object Instantiate(Object original); public static Object Instantiate(Object original, Transform parent); public static Object Instantiate(Object original, Transform parent, bool instantiateInWorldSpace); public static Object Instantiate(Object original, Vector3 position, Quaternion rotation); public static Object Instantiate(Object original, Vector3 position, Quaternion rotation, Transform parent); L\u0026rsquo;objet original est souvent un Prefab passé en paramètre du script.
Il est enfin aussi possible de détruire des GameObjects à l\u0026rsquo;exécution avec la méthode Destroy6 : public static void Destroy(Object obj, float t = 0.0F);
User Inputs#Pour une application 3D interactive, il est important de pouvoir écouter et récupérer les entrées de l\u0026rsquo;utilisateur. Dans Unity, ces entrées sont configurables en allant dans le menu Edit \u0026gt; Projects Settings : En sélectionnant Input Manager, vous obtenez une fenêtre similaire à celle montrée dans la figure suivante : Dans votre Script, il est ensuite facile de vérifier si l\u0026rsquo;utilisateur est en train de \u0026ldquo;se déplacer\u0026rdquo;, en interrogeant l\u0026rsquo;état de l\u0026rsquo;Axis comme dans le code qui suit : 1 2 3 4 5 6 7 8 9 10 11 12 13 public float speed = 1.0f; void Update() { float hdisplacement = Input.GetAxis(\u0026#34;Horizontal\u0026#34;); var displacement = new Vector3(hdisplacement, 0, 0); displacement *= speed * Time.deltaTime; // DOES NOT WORK // transform.position.x += displacement.x; // OK // transform.position += displacement; // OK transform.Translate(displacement); } Coroutines#Les Event Functions dans les Scripts sont synchrones : elles vont bloquer l\u0026rsquo;application le temps qu\u0026rsquo;elles s\u0026rsquo;exécutent. On peut rendre certaines fonctions non-bloquantes en utilisant des Coroutines7. C\u0026rsquo;est utile par exemple :
pour répartir de longs calculs sur plusieurs frames; pour simuler un timer qui va exécuter une action après un certain délai. Pour séparer les longs calculs sur plusieurs frames par exemple, il faut :
définir une Coroutine. Ceci se fait en définissant une méthode qui retourne un objet de type IEnumerator; dans la Coroutine, rajouter un point de synchronisation avec Unity pour lui rendre la main jusqu\u0026rsquo;à la prochaine frame. Ceci se fait en utilisant le mot-clef yield; enfin, il faut démarrer la Coroutine. Ces 3 étapes sont résumées dans le code qui suit : 1 2 3 4 5 6 7 8 9 10 11 12 13 void Start() { StartCoroutine(\u0026#34;LongProcess\u0026#34;); } IEnumerator LongProcess() { for (int i = 0; i \u0026lt; 100; ++i) { Debug.Log(gameObject.name + \u0026#34;: Iteration #\u0026#34; + i); yield return null; } } On peut aussi créer un timer avec un délai en utilisant la même technique, mais cette fois-ci dans le yield, on retourne un objet de type WaitForSeconds qui va suspendre l\u0026rsquo;exécution de la méthode pour un certain nombre de secondes. Par exemple, si on souhaite faire \u0026ldquo;exploser\u0026rdquo; un GameObject un certain nombre de secondes après que l\u0026rsquo;utilisateur a appuyer sur la touche du haut, alors on pourrait faire comme ce qui suit : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 private bool triggered = false; void Update() { if (Input.GetAxis(\u0026#34;Vertical\u0026#34;) \u0026gt; 0.0 \u0026amp;\u0026amp; triggered == false) { triggered = true; StartCoroutine(\u0026#34;Explode\u0026#34;); } } IEnumerator Explode() { yield return new WaitForSeconds(3.0f); var rigidBody = GetComponent\u0026lt;Rigidbody\u0026gt;(); float fx = Random.Range(-500.0f, 500.0f); float fy = Random.Range(-500.0f, 500.0f); float fz = Random.Range(-500.0f, 500.0f); rigidBody.AddForce(new Vector3(fx, fy, fz)); float tx = Random.Range(-50.0f, 50.0f); float ty = Random.Range(-50.0f, 50.0f); float tz = Random.Range(-50.0f, 50.0f); rigidBody.AddTorque(new Vector3(tx, ty, tz)); triggered = false; } Comme vu ci-dessus, il est possible de démarrer une coroutine en passant son nom sous forme de string à la méthode StartCoroutine. Cette façon de faire à l\u0026rsquo;avantage d\u0026rsquo;être simple et suffit dans la grande majorité des cas. Cependant, elle présente 2 désavantages majeurs :
Elle est plus coûteuse en temps d\u0026rsquo;exécution. On ne peut passer qu\u0026rsquo;un seul paramètre à la Coroutine. La 2ème approche consiste à passer directement la Coroutine avec ses paramètres au moment d\u0026rsquo;appeler StartCoroutine.
Enfin, il est possible d\u0026rsquo;arrêter une Coroutine avec la méthode StopCoroutine8.
L\u0026rsquo;exemple suivant montre comment démarrer et arrêter une Coroutine à la demande. Il montre aussi comment passer 2 (ou plus) paramètres à une Coroutine : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 using System.Collections; using System.Collections.Generic; using UnityEngine; public class Coroutine_101 : MonoBehaviour { public string message = \u0026#34;\u0026#34;; public float delay = 0.0f; [SerializeField] private bool running = false; private IEnumerator myCoroutine; void Start() { } void Update() { if (Input.GetAxis(\u0026#34;Vertical\u0026#34;) \u0026gt; 0.0 \u0026amp;\u0026amp; running == false) { myCoroutine = MyCoroutine(message, delay); StartCoroutine(myCoroutine); running = true; } if (Input.GetAxis(\u0026#34;Vertical\u0026#34;) \u0026lt; 0.0) { StopCoroutine(myCoroutine); running = false; } } public IEnumerator MyCoroutine(string message, float delay) { int nbIterations = 0; while(true) { yield return new WaitForSeconds(delay); Debug.Log(\u0026#34;MyCoroutine #\u0026#34; + nbIterations + \u0026#34;\\nmessage: \u0026#34; + message + \u0026#34;\\ndelay: \u0026#34; + delay); nbIterations++; } } } Remarque
Il faut noter qu\u0026rsquo;ici, on stocke le IEnumerator retourner par la Coroutine avant de la démarrer. C\u0026rsquo;est nécessaire pour pouvoir l\u0026rsquo;arrêter par la suite. Il est aussi possible d\u0026rsquo;appeler StopCoroutine en passant le nom de la Coroutine sous forme de string.
Slides#Références#Debug.Log, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Event Functions, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
MonoBehavior, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Order of execution for event functions, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Object.Instantiate, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Object.Destroy, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Coroutines, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
StopCoroutine, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:37,href:"/docs/cours/04_01-animation-de-personnages-3d/",title:"Chapitre 4.1 : animation de personnages 3D",section:"Docs",content:`Chapitre 4.1 : animation de personnages 3D#L\u0026rsquo;animation 3D englobe beaucoup de domaines, chacun avec ses propres techniques et spécificités. On peut noter par exemple :
l\u0026rsquo;animation de fluides (liquides, gaz, etc.); l\u0026rsquo;animation de visages; l\u0026rsquo;animation de particules; l\u0026rsquo;animation de corps rigides (pierres qui roulent, collisions, etc.); et l\u0026rsquo;animation de personnages. Il faut aussi noter que certaines de ces techniques sont calculables en temps-réel alors que d\u0026rsquo;autres sont trop complexes et doivent être pré-calculées. Enfin, certaines techniques sont hybrides : elles pré-calculent une partie des résultats, et les réutilisent en temps-réel pour finaliser l\u0026rsquo;animation.
La vidéo suivante par exemple montre une animation de fluide en partie pré-calculée, puis rejouée en temps-réel : Ici, les lois de la physique sont utilisées pour calculer les mouvements de fourrure en temps-réel : Enfin, la vidéo suivante montre une simulation physique de corps rigides en temps-réel : Dans cette section, nous allons nous concentrer sur l\u0026rsquo;animation de personnages en 3D et en temps-réel, comme ce qui est montré sur la vidéo suivante : Ce que nous allons faire dans cette section :
trouver des données d\u0026rsquo;animations 3D existantes; créer un système pour animer notre personnage 3D. On parle de moteur d\u0026rsquo;animation; contrôler notre moteur d\u0026rsquo;animation en fonction des événements déclenchés (entrées de l\u0026rsquo;utilisateur, collisions, etc). Trouver des données d\u0026rsquo;animations 3D#Créer des animations 3D est très compliqué et demande beaucoup de temps. En particulier, chaque personnage 3D nécessite une configuration spécifique pour l\u0026rsquo;animation 3D. Dans de nombreuses situations, il est donc trop complexe (et trop coûteux) de créer des animations 3D fabriquer spécifiquement pour notre application. Heureusement, il existe beaucoup de ressources permettant de facilement trouver, et télécharger des animations 3D utilisables dans Unity.
Unity Asset Store#L\u0026rsquo;Asset Store de Unity possède de nombreuses animations. Dans cette section par exemple, nous allons utiliser le Zombie Animation Pack Free : Nous allons en particulier utiliser le personnage 3D présent dans le répertoire Assets/ZombieAnimationPackFree/Models/Appearance/BH-2/Mesh : Mixamo#Mixamo1 est une base de données interactive contenant beaucoup d\u0026rsquo;animations 3D. C\u0026rsquo;est une excellente solution car les données présentes sont gratuites. Mixamo propose donc une excellente alternative à l\u0026rsquo;Asset Store de Unity. Créer un moteur d\u0026rsquo;animation 3D#Dans Unity, un moteur d\u0026rsquo;animation 3D est représenté par un Animator Controller2. Une fois créé, ce Component doit être connecté à l\u0026rsquo;Animator du personnage 3D que nous souhaitons animer comme le montre la figure qui suit : Un Animator Controller est en fait une machine à états finis (un graphe) qui sert à :
gérer les clips d\u0026rsquo;animation 3D (on parle d\u0026rsquo;Animation Clips3); changer l\u0026rsquo;animation courante (celle en train d\u0026rsquo;être jouée) grâce aux State Machine Transitions et aux Triggers4; mélanger les animations entre elles pour en créer de nouvelles en utilisant des Blend Trees. Les Transitions permettent de changer d\u0026rsquo;animation active :
en temps-réel; en fonction des entrées de l\u0026rsquo;utilisateur; de manière fluide, sans secousse. Pour créer un Animator Controller, il faut aller dans le menu Assets \u0026gt; Create \u0026gt; Animator Controller comme indiqué sur la figure qui suit : Il faut ensuite l\u0026rsquo;assigner à l\u0026rsquo;Animator du personnage 3D que l\u0026rsquo;on souhaite animer puis l\u0026rsquo;ouvrir dans l\u0026rsquo;éditeur (en double cliquant dessus par exemple). Vous devriez obtenir une fenêtre ressemblant à la figure suivante : État simple#Comme dit précédemment, un Animator Controller est une machine d\u0026rsquo;états finis. Chaque état représente en fait une animation du personnage 3D. Pour rajouter un nouvel état à l\u0026rsquo;Animator Controller, il suffit de faire un clic droit dans la fenêtre Animator, puis de sélectionner Create State \u0026gt; Empty : À chaque état créé, on peut associer un Animation Clip comme indiqué sur la figure suivante : Nous pouvons créer des transitions entre chaque état en faisant un clic droit sur l\u0026rsquo;état initial de la transition, en sélectionnant Make Transition, puis en sélectionnant l\u0026rsquo;état d\u0026rsquo;arrivée : Enfin, nous pouvons rajouter des Triggers aux Transitions pour les activer lorsque certains événements sont déclenchés. Pour créer un Trigger, il faut aller dans Parameters, puis cliquer sur + et choisir Trigger : Pour le rajouter à une transition, il suffit de la sélectionner. Puis dans l\u0026rsquo;Inspector, aller dans le bloc Conditions pour en rajouter une, et sélectionner le Triggers précédemment créé : En utilisant le pack d\u0026rsquo;animations 3D rajouté à notre projet, nous pouvons :
créer 3 états intitulés Idle, Hit et Attack; leur associer les animations adéquates; rajouter les transitions nécessaires pour passer de Idle à Hit (et vice-versa) et de Idle à Attack; créer les Triggers Hit et Attack et les associer aux transitions adéquates. En suivant ces étapes, ceci nous donne un graphe qui ressemble à ceci : Les Blend Trees#Les Blend Trees5 permettent de combiner des Animation Clips en temps-réel afin de générer de nouvelles animations. Par exemple, vous pouvez combiner une animation de marche lente avec une animation de marche rapide pour créer des animations de marche à des vitesses variables.
Les Blend Trees sont en fait contenus par certains états du graphe. Ils englobent plusieurs Animation Clips et un système pour interpoler entre eux. Il est possible de créer un état contenant un Blend Tree vide en faisant un clic droit dans la fenêtre Animator puis sélectionner Create State \u0026gt; From New Blend Tree : Il est aussi possible de rajouter un Blend Tree dans un état existant en faisant un clic droit sur un état existant et en choisissant Create new BlendTree in State : Si l\u0026rsquo;on crée un Blend Tree et qu\u0026rsquo;on l\u0026rsquo;ouvre en double-cliquant dessus, alors on obtient quelque chose comme ce qui suit dans l\u0026rsquo;Inspector : Dans un premier temps, il est important de choisir la façon donc le Blend Tree va interpoler les animations entre elles. Pour ceci, il existe plusieurs types d\u0026rsquo;interpolation :
1D6 : les animations sont interpolées en 1 dimension, donc selon un paramètre unique; 2D Simple Directional7 : les animations sont interpolées en 2 dimensions, donc en utilisant 2 paramètres. Certaines restrictions (vitesses des animations, directions inférieures à 180°, etc.) peuvent limiter son utilisation; 2D Freeform Directional7 : ce type d\u0026rsquo;interpolation est similaire à ce qui précède mais avec moins de contraintes. Il est cependant plus compliqué à utiliser; 2D Freeform Cartesian7 : ici l\u0026rsquo;interpolation n\u0026rsquo;est plus optimisée pour des animations de locomotion mais peut être utilisée pour interpoler toute sorte de mouvements. Direct8 : ce dernier type d\u0026rsquo;interpolation permet de gérer directement l\u0026rsquo;interpolation avec plusieurs paramètres. Les Blend Trees demandent donc des paramètres en entrée pour calculer l\u0026rsquo;interpolation. Ces paramètres sont rajoutés de manière identique aux Triggers vus précédemment. Dans le cas d\u0026rsquo;un moteur d\u0026rsquo;animation de locomotion par exemple, on peut vouloir contrôler la vitesse et la direction de déplacement. Dans ce cas, on peut créer des paramètres de type Float. Enfin, dans l\u0026rsquo;Inspector, on rajoute ces paramètres comme entrées au Blend Tree.
À ce stade, il nous reste encore à rajouter des Animation Clips. Pour ce faire, il faut aller dans la section Motion dans la fenêtre Inspector, cliquer sur le + et sélectionner Add Motion Field : À chaque Motion Field ainsi créé, on va associer des valeurs des paramètres du Blend Tree. Quand ces valeurs seront entrées dans le Blend Tree, alors l\u0026rsquo;Animation Clip sera jouer à 100%. Sinon, le Blend Tree va prendre les mouvements les plus proches (en termes de valeurs des paramètres) et les interpoler.
Pour reprendre notre exemple, nous devons donc :
créer un Blend Tree dans l\u0026rsquo;état Idle; rajouter 2 paramètres de type Float (appelés par exemple Speed et Direction) à notre Animator Controller; choisir le type 2D Freeform Direction pour le paramètre Blend Type; rajouter 4 Motion Fields pour les Animation Clips souhaités dans le Blend Tree comme par exemple un mouvement Idle, Move Forward, Move Left et Move Right; pour chaque Motion Field, spécifier les valeurs (vitesse, direction) qu\u0026rsquo;ils représentent. Par exemple, pour le mouvement Idle, alors on peut lui donner les valeur (0, 0). Pour le mouvement Move Forward les valeurs (1, 0). Pour le mouvement Move Left les valeurs (1, -1) et enfin pour le mouvement Move Right les valeurs (1, 1). Contrôler le moteur d\u0026rsquo;animations#Nous avons donc pour le moment un moteur d\u0026rsquo;animations 3D qui a plusieurs paramètres. Il nous reste enfin à le modifier pour le faire réagir en temps-réel aux différents événements de notre application 3D.
Les transitions ont des conditions qui leur sont attachées. Par exemple, la transition entre l\u0026rsquo;état Idle et l\u0026rsquo;état Attack va s\u0026rsquo;activer lorsque l\u0026rsquo;utilisateur active l\u0026rsquo;Axis nommé Fire1. Pour ce faire, il suffit d\u0026rsquo;attacher le Script suivant au personnage 3D : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 using System.Collections; using System.Collections.Generic; using UnityEngine; public class Attack : MonoBehaviour { private Animator m_animator; // Start is called before the first frame update void Start() { m_animator = GetComponent\u0026lt;Animator\u0026gt;(); } // Update is called once per frame void Update() { if (Input.GetAxis(\u0026#34;Fire1\u0026#34;) \u0026gt; 0) { m_animator.SetTrigger(\u0026#34;Attack\u0026#34;); } } } Enfin, on peut modifier les paramètres Speed et Direction quand l\u0026rsquo;utilisateur presse les touches associées aux Axis Horizontal et Vertical : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 using System.Collections; using System.Collections.Generic; using UnityEngine; public class UserControls : MonoBehaviour { private Animator m_animator; // Start is called before the first frame update void Start() { m_animator = GetComponent\u0026lt;Animator\u0026gt;(); } // Update is called once per frame void Update() { if (Input.GetAxis(\u0026#34;Fire1\u0026#34;) \u0026gt; 0) { m_animator.SetTrigger(\u0026#34;Attack\u0026#34;); } float speed = Input.GetAxis(\u0026#34;Vertical\u0026#34;); if (speed != 0) { m_animator.SetFloat(\u0026#34;Speed\u0026#34;, speed); } float direction = Input.GetAxis(\u0026#34;Horizontal\u0026#34;); if (direction != 0) { m_animator.SetFloat(\u0026#34;Direction\u0026#34;, direction); } } } Slides#Références#Mixamo\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Animator Controllers, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Animation Clips, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
State Machine Transitions, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Blend Trees, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
1D Blending, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
2D Blending, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Direct Blending, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:38,href:"/docs/cours/05-illumination-globale/",title:"Chapitre 5 : illumination globale",section:"Docs",content:`Chapitre 5 : illumination globale#Dans ce chapitre, nous allons nous intéresser aux techniques permettant d\u0026rsquo;améliorer le rendu final de notre scène 3D. Il est tout d\u0026rsquo;abord important de bien faire la différence entre 2 types d\u0026rsquo;illuminations :
l\u0026rsquo;illumination directe, qui s\u0026rsquo;intéresse aux interactions directes entre les lumières et les objets; l\u0026rsquo;illumination indirecte, qui s\u0026rsquo;intéresse aux interactions indirectes entre les lumières et les objets. Par exemple, quand une lumière illumine un objet qui à son tour influence l\u0026rsquo;illumination d\u0026rsquo;un autre objet alors on parle d\u0026rsquo;illumination indirecte.
L\u0026rsquo;illumination globale est un ensemble de techniques permettant de considérer la lumière indirecte c\u0026rsquo;est-à-dire celle qui est donc réfléchie par les différentes surfaces de la scène 3D (autres que les lumières). Ces algorithmes sont pour la plupart difficiles à implémenter et surtout coûteux en temps de calcul.
L\u0026rsquo;idée générale est de tirer partie des informations de la scène 3D pour effectuer (et stocker) le plus de calculs possibles (pré-calculs) relatifs à la lumière, avant de lancer l\u0026rsquo;application. Ces informations précalculées sont appelées des Lightmaps. De plus, quand on précalcule on dit souvent qu\u0026rsquo;on Bake. Ces Lightmaps sont ensuite réutilisées en temps-réel pour améliorer l\u0026rsquo;illumination de la scène 3D.
La figure suivante montre la différence entre une scène 3D qui ne considère que l\u0026rsquo;illumination directe (avec des réflexions) à gauche et une scène qui prend aussi en compte l\u0026rsquo;illumination globale : Illumination directe (avec réflexions)
Illumination globale ajoutée
On peut en particulier noter le sol vert qui \u0026ldquo;illumine\u0026rdquo; les murs blancs. On voit aussi que les intersections des murs sont plus ombragées.
Il est cependant important de prendre en compte une contrainte forte : le tout doit être calculable en temps-réel, c\u0026rsquo;est à dire au minimum 30 fois par secondes. Ainsi, pour qu\u0026rsquo;une application 3D soit considérée comme temps-réelle, il faut que chaque image soit calculable en moins de 16.67ms. Pour comparaison, le film Coco de Pixar a demandé pour les scènes les plus complexes 55 heures de calcul par image. Il est donc important de comprendre que le niveau de réalisme visé n\u0026rsquo;est pas comparable à celui atteint pour les films (qui sont eux entièrement précalculés).
Lighting Settings PREVIEW et FINAL#Le calcul de ces Lightmaps se fait dans l\u0026rsquo;éditeur de Unity directement. Cependant, si les Lightmaps sont très détaillées, alors elles vont demander beaucoup de temps de précalcul mais donneront un résultat réaliste à l\u0026rsquo;exécution. C\u0026rsquo;est ce que nous souhaitons dans la version finale de notre application. Au contraire, si elles sont peu détaillées, elles seront rapides à précalculer mais donneront des résultats plus grossiers à l\u0026rsquo;exécution. C\u0026rsquo;est ce que nous souhaitons lorsque l\u0026rsquo;on travaille encore sur l\u0026rsquo;application et que l\u0026rsquo;on souhaite rapidement pré-visualiser les résultats.
Nous allons donc créer 2 Lighting Settings. Pour ce faire, il faut aller dans menu Window \u0026gt; Rendering \u0026gt; Lighting : Ensuite, en cliquant sur le bouton New Lighting Settings, nous pouvons définir des valeurs de paramètres pour l\u0026rsquo;illumination.
Voici 2 exemples de configurations pour la prévisualisation et le rendu final : Configuration pour la prévisualisation
Configuration pour le rendu final
Notez que l\u0026rsquo;on passe de 8 secondes pour précalculer les Lightmaps en mode prévisualisation à plus de 5 minutes en mode rendu final.
Objets 3D statiques#Si les objets de la scène sont mobiles (donc dynamiques), la lumière changera aussi de manière dynamique et rien (ou très peu) ne pourra être précalculé. Au contraire, si les objets 3D sont immobiles (statiques) dans la scène, alors on peut utiliser cette information pour précalculer leur illumination.
Il faut donc marquer les objets immobiles comme static dans Unity pour qu\u0026rsquo;ils soient pris en compte dans le calcul de l\u0026rsquo;illumination globale : Lumières statiques et lumières dynamiques#De la même manière que pour les objets 3D, les lumières peuvent être statiques, ou dynamiques. Dans Unity, les Lights peuvent en effet avoir 3 modes 1:
Realtime (dynamique) : cette Light est calculée en temps-réel uniquement et ne contribue pas à l\u0026rsquo;illumination globale; Baked (statique) : cette Light n\u0026rsquo;est pas calculée en temps-réel. Cependant, elle est prise en compte dans le calcul de l\u0026rsquo;illumination globale; -Mixed (mixée) : cette Light est calculée en temps-réel et est prise en compte dans le calcul de l\u0026rsquo;illumination globale. Pour chaque Light, il faut ajuster son mode en fonction des effets désirés : Occlusion Ambiante#L\u0026rsquo;occlusion ambiante (ou Ambient Occlusion) dans Unity permet de simuler l\u0026rsquo;impact sur l\u0026rsquo;illumination qu\u0026rsquo;ont des surfaces voisines. Par exemple, 2 surfaces à angle droit auront tendance un atténuer la lumière arrivant à la jonction. L\u0026rsquo;Ambient Occlusion peut être précalculée en modifiant les paramètres suivants : Il faut noter qu\u0026rsquo;il est possible d\u0026rsquo;estimer l\u0026rsquo;Ambient Occlusion lors de la phase de post-processing, donc en temps-réel. Ce sujet n\u0026rsquo;est pas couvert dans ce chapitre.
Light Probes#L\u0026rsquo;illumination globale ne concerne que les objets statiques ainsi que les Lights dont le mode est Baked ou Mixed. Dans ces conditions, un objet dynamique ne peut pas être influencé par l\u0026rsquo;illumination globale. Il existe cependant une alternative : précalculer les informations relatives à l\u0026rsquo;illumination globale à certains points dans la scène 3D. Puis réutiliser ces informations en temps-réel pour illuminer les objets dynamiques. Pour ce faire, il faut créer des Light Probes Group : Le Light Probes Group ainsi créé ressemble à ce qui suit dans l\u0026rsquo;Inspector : Il est enfin nécessaire de modifier chaque Light Probe individuellement pour les placer aux endroits nécessaires. Pour activer le mode d\u0026rsquo;édition, il faut d\u0026rsquo;abord cliquer sur le bouton Edit Light Probes.
Slides#Références#Light Mode, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:39,href:"/docs/cours/06-post-processing/",title:"Chapitre 6 : post-processing",section:"Docs",content:`Chapitre 6 : post-processing#Les effets de Post-Processing1 sont appliqués à l\u0026rsquo;image rendue par la caméra dans la scène. Ils sont très variés et permettent d\u0026rsquo;améliorer significativement le rendu final. Il est par exemple possible de rajouter du flou de bougé (Motion Blur), d\u0026rsquo;échantillonner les couleurs, ou encore de rajouter un effet de vignettage.
Les effets de Post-Processing dépendent évidemment de l\u0026rsquo;image finale rendue. Mais ils dépendent aussi de la position de la caméra dans la scène : il est ainsi possible d\u0026rsquo;activer certains effets en fonction de la position courante de cette dernière.
Pour activer le calcul des effets de Post-Processing pour la caméra qui nous intéresse, il faut sélectionner notre Camera, et dans la section Rendering, activer le paramètre Post Processing : Les effets de Post-Processing sont attachés à un Volume : quand la caméra entre dans ce Volume, les effets sont appliqués. Pour commencer, nous pouvons créer un Global Volume : dans ce cas, les effets attachés à ce Volume sont toujours appliqués à la caméra. Il faut donc aller dans le menu GameObjects \u0026gt; Volume \u0026gt; Global Volume : Tous les paramètres de tous les effets de Post-Processing d\u0026rsquo;un Volume sont stockés dans un Profile. Il faut donc, dans un premier temps, créer un Profile et l\u0026rsquo;attacher au Volume que nous venons de créer. Pour ce faire, il suffit d\u0026rsquo;aller sur le paramètre Profile de notre Volume et de cliquer sur le bouton New : Le Profile est automatiquement créé et attaché au Volume. De plus, tous les paramètres sont automatiquement initialisés avec des valeurs par défaut, et sont tous initialement cachés.
Pour modifier les paramètres des effets de Post-Processing, il faut donc ajouter des Overrides au Profile. Ceci se fait en cliquant sur le bouton Add Override dans le Volume : Il y a un Override par effet de Post-Processing. Il existe beaucoup d\u0026rsquo;effets disponibles et ils dépendent du pipeline de rendu utilisé. La vidéo suivante détaille la plupart des effets et leur impact sur le rendu final de la caméra pour le pipeline de rendu URP :
Il est possible de modifier le mode de fonctionnement du Volume utilisé. Il faut pour cela éditer le paramètre Mode du Volume : Une fois le Volume en mode Local, il faut lui attacher un Collider : Maintenant, quand la caméra entrera dans l\u0026rsquo;espace défini par ce Collider, les effets de Post-Processing seront appliqués.
Slides#Références#Post-processing in the Universal Render Pipeline, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:40,href:"/docs/cours/07-shadergraph/",title:"Chapitre 7 : Shader Graph",section:"Docs",content:`Chapitre 7 : Shader Graph#IMPORTANT
Le Shader Graph est compatible avec le Universal Render Pipeline et le High-Definition Render Pipeline. Il n\u0026rsquo;est pas compatible avec le Built-in Render Pipeline.Dans une application 3D, il est primordial de modifier le rendu visuel final de certains objets. Ceci se fait en écrivant des Shaders. Pour cela, Unity propose un langage spécifique qui s\u0026rsquo;appelle ShaderLab. ShaderLab permet en particulier de facilement et rapidement déclarer des aspects importants de nos Shaders comme les passes de rendu, certaines configurations ou la gestion de la transparence. Le corps des shaders est lui écrit en HLSL (pour High-Level Shader Language)1.
De plus, Unity propose un outil qui s\u0026rsquo;appelle le Shader Graph2. Le Shader Graph permet de créer des Shaders de manière visuelle. Les opérations sont représentées par des Nodes, les données en entrée par des Input Ports, et les données en sortie par des Output Ports. Pour définir comment les données transitent d\u0026rsquo;un Node vers les autres, les Ports sont connectées entre eux. Enfin, un Shader dans le Shader Graph peut être paramétré par des Properties. Ces Properties sont visibles et éditables directement dans la fenêtre Inspector de l\u0026rsquo;éditeur. Un Shader créé avec le Shader Graph est un Asset. Il faut donc le créer directement dans la fenêtre Project ou dans le menu Asset. Ensuite, il faut aller dans le menu Create \u0026gt; Shader puis choisir le type de Shader que nous souhaitons créer. Une fois créé, nous pouvons ouvrir notre Shader directement dans le Shader Graph en double-cliquant dessus. La fenêtre suivante apparaît : Nous pouvons noter les éléments suivants :
la fenêtre Blackboard qui contient la liste des Properties du Shader, c\u0026rsquo;est-à-dire l\u0026rsquo;ensemble des paramètres du Shader qui seront modifiables directement depuis l\u0026rsquo;éditeur. Pour le moment, elle est vide; la fenêtre Graph Inspector : elle fonctionne de la même manière que la fenêtre Inspector que nous avons déjà utilisée dans l\u0026rsquo;éditeur. En particulier, elle affiche les informations importantes concernant les éléments actuellement sélectionnés dans le graph; le Shader représenté par un graphe. Pour le moment, il ne contient que la Master Stack. La Master Stack contient en fait le Vertex Shader et le Fragment Shader. Ils représentent les sorties effectives de notre Shader. le Main Preview qui permet de voir directement un aperçu des résultats produits par notre Shader. ATTENTION
L\u0026rsquo;emplacement ainsi que la visibilité des fenêtres peuvent varier.
Pour le moment, notre Shader ne fait pas grand chose. Nous allons donc rajouter des Nodes, puis établir les connexions nécessaires entre leurs Ports respectifs. Pour créer un nouveau Node, il suffit de faire un clic droit puis de sélectionner unity Create Node : La fenêtre qui apparaît vous affiche la liste de tous les Nodes actuellement disponibles pour le Shader Graph. Il en existe déjà beaucoup et chaque nouvelle version de Unity en intègre de nouveaux. Pour connecter des Ports entre eux, il suffit de sélectionner l\u0026rsquo;Output Port du Node de départ, et le glisser-déposer sur l\u0026rsquo;Input Port du Node d\u0026rsquo;arrivée.
Par exemple, on peut :
créer un Node de type Voronoi; créer un Node de type Color; multiplier leurs sorties avec un Node de type Multiply et connecter son Output Port à l\u0026rsquo;Input Port Base Color du Fragment Shader; Enfin, on peut faire varier l\u0026rsquo;Input Port Angle Offset du Node Voronoi grâce à un Node de type Time. Une fois ces étapes effectuées, on obtient le Shader Graph suivant : Pour tester le résultat dans notre scène, il nous faut enfin :
enregistrer l\u0026rsquo;Asset en cliquant sur le bouton Save Asset dans la barre d\u0026rsquo;outils; créer un nouveau Material et lui assigner le Shader créé; créer un object et lui affecter le Material. Au final, nous obtenons un résultat similaire à ce qui suit : IMPORTANT Il ne faut pas oublier de sauvegarder son Shader en cliquant sur le bouton Save asset pour voir le résultat dans l\u0026rsquo;éditeur.Nous avons donc pour le moment créé un Shader qui permet de modifier l\u0026rsquo;aspect des objects auxquels il est attaché. Cependant, il n\u0026rsquo;est pas vraiment flexible. En particulier, nous aimerions pouvoir modifier la couleur finale, ainsi que la vitesse d\u0026rsquo;animation de notre Shader. Pour ce faire, nous pouvons créer des Properties. Ces Properties seront ensuite modifiables depuis l\u0026rsquo;éditeur, on directement dans un script. Pour cela, nous devons soit :
créer de nouvelles Properties dans la fenêtre Blackboard en cliquant sur le bouton \u0026#43; et en choisissant le type de Property désiré; convertir un Node de type Input existant dans le Shader Graph. En convertissant le Node de type Color, en rajoutant les Properties speed et density de type Float, et en ajustant les connexions existantes, on obtient le Shader Graph suivant : Maintenant, nous pouvons voir le résultat dans la scène Unity directement et surtout, nous pouvons modifier les Properties pour produire des effets variés : INFORMATIONS
Pour les Properties de type Float, on peut spécifier de les afficher avec des Sliders en modifiant leurs paramètres dans la fenêtre Graph Inspector.
La couleur en entrée peut enfin être remplacée par une texture. Dans ce cas, il faut tout simplement créer une Property de type Texture2D. Ensuite, il faut créer un Node de type Sample Texture 2D qui va être en charge d\u0026rsquo;échantillonner la texture et de retourner les bonnes couleurs, en fonction des UV passées en entrée.
EXERCICE
Rajouter la possibilité d\u0026rsquo;utiliser une texture en entrée de notre Shader Graph.
Slides#Références#High-level shader language (HLSL), Microsoft\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Shader Graph, Unity\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:41,href:"/posts/coco-de-pixar-quelques-chiffres/",title:"Coco de Pixar : quelques chiffres",section:"Articles",content:`Cet article résume les points les plus intéressants mentionnés durant l\u0026rsquo;interview donnée par Renee Tam de Pixar à Foundry le 25 octobre 2017 1.
Le film Coco, créé par Pixar et sorti en 2017 est le dernier grand succès de Pixar2.
Le film Coco contient 8.7 millions de lumières, 27 millions d\u0026rsquo;objets et 1 milliards d\u0026rsquo;assets.
Les images les plus complexes ont demandé jusqu\u0026rsquo;à 450 heures de calcul pour le rendu.
En implémentant certaines techniques d\u0026rsquo;optimisation, en particulier en pré-calculant certaines parties de l\u0026rsquo;arrière-plan, il a été possible de descendre les temps de rendu à 55 heures par image.
Références#One billion assets: How Pixar’s Lightspeed team tackled Coco’s complexity, Foundry\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Coco (2017), IMDB\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:43,href:"/posts/photorealism-explained/",title:"Photoréalisme : les points importants",section:"Articles",content:`Cet article résume les points les plus intéressants mentionnés dans les vidéos ci-dessous :
75% des images des catalogues IKEA sont en fait des images de synthèse photoréalistes (au lieu de photographies). Les images de synthèse offrent une plus grande flexibilité. Elles permettent surtout de visualiser des effets et des objets qui n\u0026rsquo;existent tout simplement pas dans la réalité.
Pour obtenir des images photoréalistes, il faut 4 éléments fondamentaux : modélisation, matériaux, illumination et post-processing. Ces 4 éléments doivent être d\u0026rsquo;une excellente qualité pour obtenir des résultats convaincants. De ces 4 domaines, les parties qui demandent le plus de temps et d\u0026rsquo;attention sont la création des matériaux, et l\u0026rsquo;illumination.
\u0026hellip;90% of the time, the materials and lighting carry the weight of creating truly photorealism imagery. - Alex Roman, From Bits to Lens
Modélisation#Utiliser les vraies dimensions des objets; Éviter les arêtes trop fines, comme les cubes créés par défaut dans Unity par exemple; Utiliser des objets réels (références) pour modéliser les objets 3D; Pour modéliser des humains, il faut avoir d\u0026rsquo;excellentes connaissances en anatomie. Matériaux#Utiliser des Materials / Shaders physiquement justes; Utiliser des Normal Maps / Bump Maps pour simuler les détails d\u0026rsquo;une surface; Simuler les imperfections des matériaux (verres, empreintes, poussière) avec des textures. Illumination#Utiliser des lumières correspondant à des lumières réelles : par exemple, pour simuler le soleil, il est préférable d\u0026rsquo;essayer de lui donner la couleur et la direction réelle; Bien gérer les réflexions des matériaux. Post-Processing#Simuler les light glares / lens flares; Simuler le Motion Blur; Bien gérer la profondeur de champs; Ajouter des aberrations chromatiques; Simuler les distorsions des lentilles des appareils photo. Gestion des couleurs / Dynamic range#La figure suivante (from Blender Guru) montre une estimation de la sensibilité des \u0026ldquo;capteurs photographiques\u0026rdquo; courants : Il faut faire attention à ne pas utiliser sRGB (ancienne technique avec une dynamique limitée) et utiliser ACES qui est un standard de l\u0026rsquo;industrie.
`})})()